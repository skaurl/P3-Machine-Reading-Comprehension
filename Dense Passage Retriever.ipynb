{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Dense Passage Retriever.ipynb","provenance":[],"collapsed_sections":["inCOuTYF45wM","y9EOJgUt5LSZ","Blrv_zj55m1A","kesHeGqw6pq6","5ZWvd9rc6rux","Jq3j0X1FLFfv","wI9v06l6PJnJ","mjvaDGJAO3Sv","YOqdpBj3O7bc"],"machine_shape":"hm","mount_file_id":"1CYut1f4OiyAUxWW-Rs1EuFGGFX-78me0","authorship_tag":"ABX9TyOxo8hVyZe3EHfYXQ2oA812"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"inCOuTYF45wM"},"source":["# Install"]},{"cell_type":"code","metadata":{"id":"kVafNZfs42fg"},"source":["!pip install datasets\n","!pip install konlpy\n","!pip install pororo\n","!pip install python-mecab-ko\n","!pip install rank_bm25"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WQQO-MbW5BPv"},"source":["!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-gYQH2fb5BpY"},"source":["cd Mecab-ko-for-Google-Colab"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u4SA44Ex5C17"},"source":["!bash install_mecab-ko_on_colab190912.sh"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y9EOJgUt5LSZ"},"source":["# Library"]},{"cell_type":"code","metadata":{"id":"pZac7ebm5OCC"},"source":["import zipfile\n","from datasets import load_from_disk\n","import json\n","import pandas as pd\n","import re\n","from tqdm import tqdm\n","from konlpy.tag import Mecab\n","from rank_bm25 import BM25Plus\n","from pororo import Pororo"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Blrv_zj55m1A"},"source":["# Data pre-processing"]},{"cell_type":"code","metadata":{"id":"KFjhdNugYeM0"},"source":["from datasets import load_dataset\n","\n","dataset = load_dataset('squad_kor_v1')\n","\n","dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xEwLDo0E5vsT"},"source":["mecab = Mecab()\n","tokenizer = Pororo(task=\"tokenization\", lang=\"ko\", model=\"mecab.bpe64k.ko\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CCDDMOcMtzBG"},"source":["train_dataset = pd.DataFrame({'title' : dataset['train']['title'],\n","                              'context' : dataset['train']['context'],\n","                              'question' : dataset['train']['question'],\n","                              'answers' : dataset['train']['answers']})\n","\n","train_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4FgEevM6ty3m"},"source":["dev_dataset = pd.DataFrame({'title' : dataset['validation']['title'],\n","                              'context' : dataset['validation']['context'],\n","                              'question' : dataset['validation']['question'],\n","                              'answers' : dataset['validation']['answers']})\n","\n","dev_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NDLD7U-NlRo8"},"source":["df = pd.concat([train_dataset,dev_dataset])\n","\n","df = df.reset_index()\n","\n","del df['index']\n","\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wue6_OwfZSlR"},"source":["corpus = df['context'].drop_duplicates().to_list()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HiICg2sGuqzh"},"source":["corpus_to_id = {corpus[num]:num for num in range(len(corpus))}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ART3XRl5zJj"},"source":["tokenized_corpus = [tokenizer(i) for i in corpus]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zZCVwFFV51uS"},"source":["bm25 = BM25Plus(tokenized_corpus)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kesHeGqw6pq6"},"source":["# Train Dataset"]},{"cell_type":"code","metadata":{"id":"iSKppvK4-tGI"},"source":["dpr_train = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DNlAjsAI-nfV"},"source":["for num in tqdm(range(len(train_dataset))):\n","    data_num = {}\n","\n","    bm25_score = bm25.get_scores(tokenizer(train_dataset['question'][num]))\n","\n","    bm25_score_sorted = sorted([(i,bm25_score[i]) for i in range(len(bm25_score))],key=lambda x : x[1])\n","    \n","    # dataset\n","    data_num['dataset'] = 'dpr_train'\n","\n","    # question\n","    data_num['question'] = train_dataset['question'][num]\n","\n","    # answers\n","    data_num['answers'] = train_dataset['answers'][num]['text']\n","\n","    # positive_ctxs\n","    data_num['positive_ctxs'] = [{\n","        'title' : train_dataset['title'][num],\n","        'text' : train_dataset['context'][num],\n","        'score' : bm25_score[corpus_to_id[train_dataset['context'][num]]],\n","        'title_score' : 0,\n","        'passage_id' : corpus_to_id[train_dataset['context'][num]]\n","    }]\n","\n","    # negative_ctxs\n","\n","    negative_ctxs_tmp = []\n","\n","    for neg,scr in bm25_score_sorted[:3]:\n","        negative_ctxs_tmp.append({\n","            'title' : '',\n","            'text' : corpus[neg],\n","            'score' : scr,\n","            'title_score' : 0,\n","            'passage_id' : neg\n","        })\n","\n","    data_num['negative_ctxs'] = negative_ctxs_tmp\n","\n","    # hard_negative_ctxs\n","\n","    hard_negative_ctxs_tmp = []\n","\n","    for hrd,scr in bm25_score_sorted[-6:]:\n","        if hrd != num:\n","            hard_negative_ctxs_tmp.append({\n","                'title' : '',\n","                'text' : corpus[hrd],\n","                'score' : scr,\n","                'title_score' : 0,\n","                'passage_id' : hrd\n","            })\n","\n","    data_num['hard_negative_ctxs'] = hard_negative_ctxs_tmp\n","\n","    dpr_train.append(data_num)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"klvQA0t9H0jN"},"source":["with open('/content/drive/MyDrive/Colab Notebooks/train/dpr_train.json', 'w', encoding=\"utf-8\") as f:\n","    json.dump(dpr_train, f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5ZWvd9rc6rux"},"source":["# Dev Dataset"]},{"cell_type":"code","metadata":{"id":"TsWqi5uSIQh8"},"source":["dpr_dev = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RI50MSsRISWG"},"source":["for num in tqdm(range(len(dev_dataset))):\n","    data_num = {}\n","\n","    bm25_score = bm25.get_scores(tokenizer(dev_dataset['question'][num]))\n","\n","    bm25_score_sorted = sorted([(i,bm25_score[i]) for i in range(len(bm25_score))],key=lambda x : x[1])\n","    \n","    # dataset\n","    data_num['dataset'] = 'dpr_dev'\n","\n","    # question\n","    data_num['question'] = dev_dataset['question'][num]\n","\n","    # answers\n","    data_num['answers'] = dev_dataset['answers'][num]['text']\n","\n","    # positive_ctxs\n","    data_num['positive_ctxs'] = [{\n","        'title' : dev_dataset['title'][num],\n","        'text' : dev_dataset['context'][num],\n","        'score' : bm25_score[corpus_to_id[dev_dataset['context'][num]]],\n","        'title_score' : 0,\n","        'passage_id' : corpus_to_id[dev_dataset['context'][num]]\n","    }]\n","\n","    # negative_ctxs\n","\n","    negative_ctxs_tmp = []\n","\n","    for neg,scr in bm25_score_sorted[:3]:\n","        negative_ctxs_tmp.append({\n","            'title' : '',\n","            'text' : corpus[neg],\n","            'score' : scr,\n","            'title_score' : 0,\n","            'passage_id' : neg\n","        })\n","\n","    data_num['negative_ctxs'] = negative_ctxs_tmp\n","\n","    # hard_negative_ctxs\n","\n","    hard_negative_ctxs_tmp = []\n","\n","    for hrd,scr in bm25_score_sorted[-6:]:\n","        if hrd != num:\n","            hard_negative_ctxs_tmp.append({\n","                'title' : '',\n","                'text' : corpus[hrd],\n","                'score' : scr,\n","                'title_score' : 0,\n","                'passage_id' : hrd\n","            })\n","\n","    data_num['hard_negative_ctxs'] = hard_negative_ctxs_tmp\n","\n","    dpr_dev.append(data_num)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dh2lp-xZI7ss"},"source":["with open('/content/drive/MyDrive/Colab Notebooks/dev/dpr_dev.json', 'w', encoding=\"utf-8\") as f:\n","    json.dump(dpr_dev, f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jq3j0X1FLFfv"},"source":["# Dense Passage Retriever Training"]},{"cell_type":"code","metadata":{"id":"wRatXJtAFMBD"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uY6qAi0TLLkK"},"source":["!pip install git+https://github.com/deepset-ai/haystack.git\n","!pip install urllib3==1.25.4\n","!python -m pip install elasticsearch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iu_qHrflLXKj"},"source":["from haystack.retriever.dense import DensePassageRetriever\n","from haystack.preprocessor.utils import fetch_archive_from_http"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3wVHPhkkFTjy"},"source":["! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n","! tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n","! chown -R daemon:daemon elasticsearch-7.9.2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oQwIYwphFW_t"},"source":["import os\n","from subprocess import Popen, PIPE, STDOUT\n","es_server = Popen(['elasticsearch-7.9.2/bin/elasticsearch'],\n","                   stdout=PIPE, stderr=STDOUT,\n","                   preexec_fn=lambda: os.setuid(1)\n","                  )\n","\n","! sleep 30"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SAZCwg_wEAyN"},"source":["! /content/elasticsearch-7.9.2/bin/elasticsearch-plugin install analysis-nori"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"knooHjO3EB5f"},"source":["es_server.kill()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QyROjYa_EFGF"},"source":["import os\n","from subprocess import Popen, PIPE, STDOUT\n","es_server = Popen(['elasticsearch-7.9.2/bin/elasticsearch'],\n","                   stdout=PIPE, stderr=STDOUT,\n","                   preexec_fn=lambda: os.setuid(1)\n","                  )\n","\n","! sleep 30"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rg3zmXBIoWTz"},"source":["from elasticsearch import Elasticsearch\n","\n","es = Elasticsearch(\"localhost:9200\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OAPg19Nno032"},"source":["es.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c9NFib1LFXz5"},"source":["from haystack.document_store.elasticsearch import ElasticsearchDocumentStore\n","document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"document\", analyzer='nori')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_77DyFsBo-EA"},"source":["es.indices.get('document')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"95-3dyhjx8tB"},"source":["# f = zipfile.ZipFile('/content/drive/MyDrive/Colab Notebooks/data.zip')\n","# f.extractall('/content')\n","# f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YXfqq1NYGB_j"},"source":["# with open('/content/data/wikipedia_documents.json', 'r') as f:\n","#     wiki_data = json.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RnF0tB-JypMx"},"source":["dicts = [{'text':df['context'][num],'meta':{'name':df['title'][num]}} for num in range(len(df))]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y4NZQuKYGCoq"},"source":["# dicts = [{'text':wiki_data[str(i)]['text'],'meta':{'name':wiki_data[str(i)]['title']}} for i in range(len(wiki_data))]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BTMyB1NWGD9p"},"source":["document_store.write_documents(dicts)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2SxovDdNLYw5"},"source":["doc_dir = \"/content/drive/MyDrive/Colab Notebooks/\"\n","\n","train_filename = \"train/dpr_train.json\"\n","dev_filename = \"dev/dpr_dev.json\"\n","\n","query_model = \"voidful/dpr-question_encoder-bert-base-multilingual\"\n","passage_model = \"voidful/dpr-ctx_encoder-bert-base-multilingual\"\n","\n","save_dir = \"/content/drive/MyDrive/Colab Notebooks/saved_models/dpr\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wI9v06l6PJnJ"},"source":["## LanguageModel"]},{"cell_type":"code","metadata":{"id":"KW2-5ZqEM_LZ"},"source":["# coding=utf-8\n","# Copyright 2018 The Google AI Language Team Authors,  The HuggingFace Inc. Team and deepset Team.\n","# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\"\"\" Acknowledgements: Many of the modeling parts here come from the great transformers repository: https://github.com/huggingface/transformers.\n","Thanks for the great work! \"\"\"\n","\n","from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","import json\n","import logging\n","import os\n","import io\n","from pathlib import Path\n","from collections import OrderedDict\n","\n","from dotmap import DotMap\n","from tqdm import tqdm\n","import copy\n","import numpy as np\n","import torch\n","from torch import nn\n","\n","logger = logging.getLogger(__name__)\n","\n","from transformers import (\n","    BertModel, BertConfig,\n","    RobertaModel, RobertaConfig,\n","    XLNetModel, XLNetConfig,\n","    AlbertModel, AlbertConfig,\n","    XLMRobertaModel, XLMRobertaConfig,\n","    DistilBertModel, DistilBertConfig,\n","    ElectraModel, ElectraConfig,\n","    CamembertModel, CamembertConfig\n",")\n","\n","from transformers import AutoModel, AutoConfig\n","from transformers.modeling_utils import SequenceSummary\n","from transformers.models.bert.tokenization_bert import load_vocab\n","import transformers\n","\n","from farm.modeling import wordembedding_utils\n","from farm.modeling.wordembedding_utils import s3e_pooling\n","\n","# These are the names of the attributes in various model configs which refer to the number of dimensions\n","# in the output vectors\n","OUTPUT_DIM_NAMES = [\"dim\", \"hidden_size\", \"d_model\"]\n","\n","\n","class LanguageModel(nn.Module):\n","    \"\"\"\n","    The parent class for any kind of model that can embed language into a semantic vector space. Practically\n","    speaking, these models read in tokenized sentences and return vectors that capture the meaning of sentences\n","    or of tokens.\n","    \"\"\"\n","\n","    subclasses = {}\n","\n","    def __init_subclass__(cls, **kwargs):\n","        \"\"\" This automatically keeps track of all available subclasses.\n","        Enables generic load() or all specific LanguageModel implementation.\n","        \"\"\"\n","        super().__init_subclass__(**kwargs)\n","        cls.subclasses[cls.__name__] = cls\n","\n","    def forward(self, input_ids, padding_mask, **kwargs):\n","        raise NotImplementedError\n","\n","    @classmethod\n","    def from_scratch(cls, model_type, vocab_size):\n","        if model_type.lower() == \"bert\":\n","            model = Bert\n","        return model.from_scratch(vocab_size)\n","\n","    @classmethod\n","    def load(cls, pretrained_model_name_or_path, revision=None, n_added_tokens=0, language_model_class=None, **kwargs):\n","        \"\"\"\n","        Load a pretrained language model either by\n","\n","        1. specifying its name and downloading it\n","        2. or pointing to the directory it is saved in.\n","\n","        Available remote models:\n","\n","        * bert-base-uncased\n","        * bert-large-uncased\n","        * bert-base-cased\n","        * bert-large-cased\n","        * bert-base-multilingual-uncased\n","        * bert-base-multilingual-cased\n","        * bert-base-chinese\n","        * bert-base-german-cased\n","        * roberta-base\n","        * roberta-large\n","        * xlnet-base-cased\n","        * xlnet-large-cased\n","        * xlm-roberta-base\n","        * xlm-roberta-large\n","        * albert-base-v2\n","        * albert-large-v2\n","        * distilbert-base-german-cased\n","        * distilbert-base-multilingual-cased\n","        * google/electra-small-discriminator\n","        * google/electra-base-discriminator\n","        * google/electra-large-discriminator\n","        * facebook/dpr-question_encoder-single-nq-base\n","        * facebook/dpr-ctx_encoder-single-nq-base\n","\n","        See all supported model variations here: https://huggingface.co/models\n","\n","        The appropriate language model class is inferred automatically from model config\n","        or can be manually supplied via `language_model_class`.\n","\n","        :param pretrained_model_name_or_path: The path of the saved pretrained model or its name.\n","        :type pretrained_model_name_or_path: str\n","        :param revision: The version of model to use from the HuggingFace model hub. Can be tag name, branch name, or commit hash.\n","        :type revision: str\n","        :param language_model_class: (Optional) Name of the language model class to load (e.g. `Bert`)\n","        :type language_model_class: str\n","\n","        \"\"\"\n","        kwargs[\"revision\"] = revision\n","        logger.info(\"\")\n","        logger.info(\"LOADING MODEL\")\n","        logger.info(\"=============\")\n","        config_file = Path(pretrained_model_name_or_path) / \"language_model_config.json\"\n","        if os.path.exists(config_file):\n","            logger.info(f\"Model found locally at {pretrained_model_name_or_path}\")\n","            # it's a local directory in FARM format\n","            config = json.load(open(config_file))\n","            language_model = cls.subclasses[config[\"name\"]].load(pretrained_model_name_or_path)\n","        else:\n","            logger.info(f\"Could not find {pretrained_model_name_or_path} locally.\")\n","            logger.info(f\"Looking on Transformers Model Hub (in local cache and online)...\")\n","            if language_model_class is None:\n","                language_model_class = cls.get_language_model_class(pretrained_model_name_or_path)\n","\n","            if language_model_class:\n","                language_model = cls.subclasses[language_model_class].load(pretrained_model_name_or_path, **kwargs)\n","            else:\n","                language_model = None\n","\n","        if not language_model:\n","            raise Exception(\n","                f\"Model not found for {pretrained_model_name_or_path}. Either supply the local path for a saved \"\n","                f\"model or one of bert/roberta/xlnet/albert/distilbert models that can be downloaded from remote. \"\n","                f\"Ensure that the model class name can be inferred from the directory name when loading a \"\n","                f\"Transformers' model. Here's a list of available models: \"\n","                f\"https://farm.deepset.ai/api/modeling.html#farm.modeling.language_model.LanguageModel.load\"\n","            )\n","        else:\n","            logger.info(f\"Loaded {pretrained_model_name_or_path}\")\n","\n","        # resize embeddings in case of custom vocab\n","        if n_added_tokens != 0:\n","            # TODO verify for other models than BERT\n","            model_emb_size = language_model.model.resize_token_embeddings(new_num_tokens=None).num_embeddings\n","            vocab_size = model_emb_size + n_added_tokens\n","            logger.info(\n","                f\"Resizing embedding layer of LM from {model_emb_size} to {vocab_size} to cope with custom vocab.\")\n","            language_model.model.resize_token_embeddings(vocab_size)\n","            # verify\n","            model_emb_size = language_model.model.resize_token_embeddings(new_num_tokens=None).num_embeddings\n","            assert vocab_size == model_emb_size\n","\n","        return language_model\n","\n","    @staticmethod\n","    def get_language_model_class(model_name_or_path):\n","        # it's transformers format (either from model hub or local)\n","        model_name_or_path = str(model_name_or_path)\n","\n","        config = AutoConfig.from_pretrained(model_name_or_path)\n","        model_type = config.model_type\n","        if model_type == \"xlm-roberta\":\n","            language_model_class = \"XLMRoberta\"\n","        elif model_type == \"roberta\":\n","            if \"mlm\" in model_name_or_path.lower():\n","                raise NotImplementedError(\"MLM part of codebert is currently not supported in FARM\")\n","            language_model_class = \"Roberta\"\n","        elif model_type == \"camembert\":\n","            language_model_class = \"Camembert\"\n","        elif model_type == \"albert\":\n","            language_model_class = \"Albert\"\n","        elif model_type == \"distilbert\":\n","            language_model_class = \"DistilBert\"\n","        elif model_type == \"bert\":\n","            language_model_class = \"Bert\"\n","        elif model_type == \"xlnet\":\n","            language_model_class = \"XLNet\"\n","        elif model_type == \"electra\":\n","            language_model_class = \"Electra\"\n","        elif model_type == \"dpr\":\n","            if config.architectures[0] == \"DPRQuestionEncoder\":\n","                language_model_class = \"DPRQuestionEncoder\"\n","            elif config.architectures[0] == \"DPRContextEncoder\":\n","                language_model_class = \"DPRContextEncoder\"\n","            elif config.archictectures[0] == \"DPRReader\":\n","                raise NotImplementedError(\"DPRReader models are currently not supported.\")\n","        else:\n","            # Fall back to inferring type from model name\n","            logger.warning(\"Could not infer LanguageModel class from config. Trying to infer \"\n","                           \"LanguageModel class from model name.\")\n","            language_model_class = LanguageModel._infer_language_model_class_from_string(model_name_or_path)\n","\n","        return language_model_class\n","\n","    @staticmethod\n","    def _infer_language_model_class_from_string(model_name_or_path):\n","        # If inferring Language model class from config doesn't succeed,\n","        # fall back to inferring Language model class from model name.\n","        if \"xlm\" in model_name_or_path.lower() and \"roberta\" in model_name_or_path.lower():\n","            language_model_class = \"XLMRoberta\"\n","        elif \"roberta\" in model_name_or_path.lower():\n","            language_model_class = \"Roberta\"\n","        elif \"codebert\" in model_name_or_path.lower():\n","            if \"mlm\" in model_name_or_path.lower():\n","                raise NotImplementedError(\"MLM part of codebert is currently not supported in FARM\")\n","            else:\n","                language_model_class = \"Roberta\"\n","        elif \"camembert\" in model_name_or_path.lower() or \"umberto\" in model_name_or_path.lower():\n","            language_model_class = \"Camembert\"\n","        elif \"albert\" in model_name_or_path.lower():\n","            language_model_class = 'Albert'\n","        elif \"distilbert\" in model_name_or_path.lower():\n","            language_model_class = 'DistilBert'\n","        elif \"bert\" in model_name_or_path.lower():\n","            language_model_class = 'Bert'\n","        elif \"xlnet\" in model_name_or_path.lower():\n","            language_model_class = 'XLNet'\n","        elif \"electra\" in model_name_or_path.lower():\n","            language_model_class = 'Electra'\n","        elif \"word2vec\" in model_name_or_path.lower() or \"glove\" in model_name_or_path.lower():\n","            language_model_class = 'WordEmbedding_LM'\n","        elif \"minilm\" in model_name_or_path.lower():\n","            language_model_class = \"Bert\"\n","        elif \"dpr-question_encoder\" in model_name_or_path.lower():\n","            language_model_class = \"DPRQuestionEncoder\"\n","        elif \"dpr-ctx_encoder\" in model_name_or_path.lower():\n","            language_model_class = \"DPRContextEncoder\"\n","        else:\n","            language_model_class = None\n","\n","        return language_model_class\n","\n","    def get_output_dims(self):\n","        config = self.model.config\n","        for odn in OUTPUT_DIM_NAMES:\n","            if odn in dir(config):\n","                return getattr(config, odn)\n","        else:\n","            raise Exception(\"Could not infer the output dimensions of the language model\")\n","\n","    def freeze(self, layers):\n","        \"\"\" To be implemented\"\"\"\n","        raise NotImplementedError()\n","\n","    def unfreeze(self):\n","        \"\"\" To be implemented\"\"\"\n","        raise NotImplementedError()\n","\n","    def save_config(self, save_dir):\n","        save_filename = Path(save_dir) / \"language_model_config.json\"\n","        with open(save_filename, \"w\") as file:\n","            setattr(self.model.config, \"name\", self.__class__.__name__)\n","            setattr(self.model.config, \"language\", self.language)\n","            string = self.model.config.to_json_string()\n","            file.write(string)\n","\n","    def save(self, save_dir):\n","        \"\"\"\n","        Save the model state_dict and its config file so that it can be loaded again.\n","\n","        :param save_dir: The directory in which the model should be saved.\n","        :type save_dir: str\n","        \"\"\"\n","        # Save Weights\n","        save_name = Path(save_dir) / \"language_model.bin\"\n","        model_to_save = (\n","            self.model.module if hasattr(self.model, \"module\") else self.model\n","        )  # Only save the model it-self\n","        torch.save(model_to_save.state_dict(), save_name)\n","        self.save_config(save_dir)\n","\n","    @classmethod\n","    def _get_or_infer_language_from_name(cls, language, name):\n","        if language is not None:\n","            return language\n","        else:\n","            return cls._infer_language_from_name(name)\n","\n","    @classmethod\n","    def _infer_language_from_name(cls, name):\n","        known_languages = (\n","            \"german\",\n","            \"english\",\n","            \"chinese\",\n","            \"indian\",\n","            \"french\",\n","            \"polish\",\n","            \"spanish\",\n","            \"multilingual\",\n","        )\n","        matches = [lang for lang in known_languages if lang in name]\n","        if \"camembert\" in name:\n","            language = \"french\"\n","            logger.info(\n","                f\"Automatically detected language from language model name: {language}\"\n","            )\n","        elif \"umberto\" in name:\n","            language = \"italian\"\n","            logger.info(\n","                f\"Automatically detected language from language model name: {language}\"\n","            )\n","        elif len(matches) == 0:\n","            language = \"english\"\n","        elif len(matches) > 1:\n","            language = matches[0]\n","        else:\n","            language = matches[0]\n","            logger.info(\n","                f\"Automatically detected language from language model name: {language}\"\n","            )\n","\n","        return language\n","\n","    def formatted_preds(self, logits, samples, ignore_first_token=True,\n","                        padding_mask=None, input_ids=None, **kwargs):\n","        \"\"\"\n","        Extracting vectors from language model (e.g. for extracting sentence embeddings).\n","        Different pooling strategies and layers are available and will be determined from the object attributes\n","        `extraction_layer` and `extraction_strategy`. Both should be set via the Inferencer:\n","        Example:  Inferencer(extraction_strategy='cls_token', extraction_layer=-1)\n","\n","        :param logits: Tuple of (sequence_output, pooled_output) from the language model.\n","                       Sequence_output: one vector per token, pooled_output: one vector for whole sequence\n","        :param samples: For each item in logits we need additional meta information to format the prediction (e.g. input text).\n","                        This is created by the Processor and passed in here from the Inferencer.\n","        :param ignore_first_token: Whether to include the first token for pooling operations (e.g. reduce_mean).\n","                                   Many models have here a special token like [CLS] that you don't want to include into your average of token embeddings.\n","        :param padding_mask: Mask for the padding tokens. Those will also not be included in the pooling operations to prevent a bias by the number of padding tokens.\n","        :param input_ids: ids of the tokens in the vocab\n","        :param kwargs: kwargs\n","        :return: list of dicts containing preds, e.g. [{\"context\": \"some text\", \"vec\": [-0.01, 0.5 ...]}]\n","        \"\"\"\n","\n","        if not hasattr(self, \"extraction_layer\") or not hasattr(self, \"extraction_strategy\"):\n","            raise ValueError(\"`extraction_layer` or `extraction_strategy` not specified for LM. \"\n","                             \"Make sure to set both, e.g. via Inferencer(extraction_strategy='cls_token', extraction_layer=-1)`\")\n","\n","        # unpack the tuple from LM forward pass\n","        sequence_output = logits[0][0]\n","        pooled_output = logits[0][1]\n","\n","        # aggregate vectors\n","        if self.extraction_strategy == \"pooled\":\n","            if self.extraction_layer != -1:\n","                raise ValueError(f\"Pooled output only works for the last layer, but got extraction_layer = {self.extraction_layer}. Please set `extraction_layer=-1`.)\")\n","            vecs = pooled_output.cpu().numpy()\n","        elif self.extraction_strategy == \"per_token\":\n","            vecs = sequence_output.cpu().numpy()\n","        elif self.extraction_strategy == \"reduce_mean\":\n","            vecs = self._pool_tokens(sequence_output, padding_mask, self.extraction_strategy, ignore_first_token=ignore_first_token)\n","        elif self.extraction_strategy == \"reduce_max\":\n","            vecs = self._pool_tokens(sequence_output, padding_mask, self.extraction_strategy, ignore_first_token=ignore_first_token)\n","        elif self.extraction_strategy == \"cls_token\":\n","            vecs = sequence_output[:, 0, :].cpu().numpy()\n","        elif self.extraction_strategy == \"s3e\":\n","            vecs = self._pool_tokens(sequence_output, padding_mask, self.extraction_strategy,\n","                                     ignore_first_token=ignore_first_token,\n","                                     input_ids=input_ids, s3e_stats=self.s3e_stats)\n","        else:\n","            raise NotImplementedError\n","\n","        preds = []\n","        for vec, sample in zip(vecs, samples):\n","            pred = {}\n","            pred[\"context\"] = sample.clear_text[\"text\"]\n","            pred[\"vec\"] = vec\n","            preds.append(pred)\n","        return preds\n","\n","    def _pool_tokens(self, sequence_output, padding_mask, strategy, ignore_first_token, input_ids=None, s3e_stats=None):\n","\n","        token_vecs = sequence_output.cpu().numpy()\n","        # we only take the aggregated value of non-padding tokens\n","        padding_mask = padding_mask.cpu().numpy()\n","        ignore_mask_2d = padding_mask == 0\n","        # sometimes we want to exclude the CLS token as well from our aggregation operation\n","        if ignore_first_token:\n","            ignore_mask_2d[:, 0] = True\n","        ignore_mask_3d = np.zeros(token_vecs.shape, dtype=bool)\n","        ignore_mask_3d[:, :, :] = ignore_mask_2d[:, :, np.newaxis]\n","        if strategy == \"reduce_max\":\n","            pooled_vecs = np.ma.array(data=token_vecs, mask=ignore_mask_3d).max(axis=1).data\n","        if strategy == \"reduce_mean\":\n","            pooled_vecs = np.ma.array(data=token_vecs, mask=ignore_mask_3d).mean(axis=1).data\n","        if strategy == \"s3e\":\n","            input_ids = input_ids.cpu().numpy()\n","            pooled_vecs = s3e_pooling(token_embs=token_vecs,\n","                                      token_ids=input_ids,\n","                                      token_weights=s3e_stats[\"token_weights\"],\n","                                      centroids=s3e_stats[\"centroids\"],\n","                                      token_to_cluster=s3e_stats[\"token_to_cluster\"],\n","                                      svd_components=s3e_stats.get(\"svd_components\", None),\n","                                      mask=padding_mask == 0)\n","        return pooled_vecs\n","\n","\n","class Bert(LanguageModel):\n","    \"\"\"\n","    A BERT model that wraps HuggingFace's implementation\n","    (https://github.com/huggingface/transformers) to fit the LanguageModel class.\n","    Paper: https://arxiv.org/abs/1810.04805\n","\n","    \"\"\"\n","\n","    def __init__(self):\n","        super(Bert, self).__init__()\n","        self.model = None\n","        self.name = \"bert\"\n","\n","    @classmethod\n","    def from_scratch(cls, vocab_size, name=\"bert\", language=\"en\"):\n","        bert = cls()\n","        bert.name = name\n","        bert.language = language\n","        config = BertConfig(vocab_size=vocab_size)\n","        bert.model = BertModel(config)\n","        return bert\n","\n","    @classmethod\n","    def load(cls, pretrained_model_name_or_path, language=None, **kwargs):\n","        \"\"\"\n","        Load a pretrained model by supplying\n","\n","        * the name of a remote model on s3 (\"bert-base-cased\" ...)\n","        * OR a local path of a model trained via transformers (\"some_dir/huggingface_model\")\n","        * OR a local path of a model trained via FARM (\"some_dir/farm_model\")\n","\n","        :param pretrained_model_name_or_path: The path of the saved pretrained model or its name.\n","        :type pretrained_model_name_or_path: str\n","\n","        \"\"\"\n","\n","        bert = cls()\n","        if \"farm_lm_name\" in kwargs:\n","            bert.name = kwargs[\"farm_lm_name\"]\n","        else:\n","            bert.name = pretrained_model_name_or_path\n","        # We need to differentiate between loading model using FARM format and Pytorch-Transformers format\n","        farm_lm_config = Path(pretrained_model_name_or_path) / \"language_model_config.json\"\n","        if os.path.exists(farm_lm_config):\n","            # FARM style\n","            bert_config = BertConfig.from_pretrained(farm_lm_config)\n","            farm_lm_model = Path(pretrained_model_name_or_path) / \"language_model.bin\"\n","            bert.model = BertModel.from_pretrained(farm_lm_model, config=bert_config, **kwargs)\n","            bert.language = bert.model.config.language\n","        else:\n","            # Pytorch-transformer Style\n","            bert.model = BertModel.from_pretrained(str(pretrained_model_name_or_path), **kwargs)\n","            bert.language = cls._get_or_infer_language_from_name(language, pretrained_model_name_or_path)\n","        return bert\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        segment_ids,\n","        padding_mask,\n","        **kwargs,\n","    ):\n","        \"\"\"\n","        Perform the forward pass of the BERT model.\n","\n","        :param input_ids: The ids of each token in the input sequence. Is a tensor of shape [batch_size, max_seq_len]\n","        :type input_ids: torch.Tensor\n","        :param segment_ids: The id of the segment. For example, in next sentence prediction, the tokens in the\n","           first sentence are marked with 0 and those in the second are marked with 1.\n","           It is a tensor of shape [batch_size, max_seq_len]\n","        :type segment_ids: torch.Tensor\n","        :param padding_mask: A mask that assigns a 1 to valid input tokens and 0 to padding tokens\n","           of shape [batch_size, max_seq_len]\n","        :return: Embeddings for each token in the input sequence.\n","\n","        \"\"\"\n","        output_tuple = self.model(\n","            input_ids,\n","            token_type_ids=segment_ids,\n","            attention_mask=padding_mask,\n","        )\n","        if self.model.encoder.config.output_hidden_states == True:\n","            sequence_output, pooled_output, all_hidden_states = output_tuple[0], output_tuple[1], output_tuple[2]\n","            return sequence_output, pooled_output, all_hidden_states\n","        else:\n","            sequence_output, pooled_output = output_tuple[0], output_tuple[1]\n","            return sequence_output, pooled_output\n","\n","    def enable_hidden_states_output(self):\n","        self.model.encoder.config.output_hidden_states = True\n","\n","    def disable_hidden_states_output(self):\n","        self.model.encoder.config.output_hidden_states = False\n","\n","\n","class Albert(LanguageModel):\n","    \"\"\"\n","    An ALBERT model that wraps the HuggingFace's implementation\n","    (https://github.com/huggingface/transformers) to fit the LanguageModel class.\n","\n","    \"\"\"\n","\n","    def __init__(self):\n","        super(Albert, self).__init__()\n","        self.model = None\n","        self.name = \"albert\"\n","\n","    @classmethod\n","    def load(cls, pretrained_model_name_or_path, language=None, **kwargs):\n","        \"\"\"\n","        Load a language model either by supplying\n","\n","        * the name of a remote model on s3 (\"albert-base\" ...)\n","        * or a local path of a model trained via transformers (\"some_dir/huggingface_model\")\n","        * or a local path of a model trained via FARM (\"some_dir/farm_model\")\n","\n","        :param pretrained_model_name_or_path: name or path of a model\n","        :param language: (Optional) Name of language the model was trained for (e.g. \"german\").\n","                         If not supplied, FARM will try to infer it from the model name.\n","        :return: Language Model\n","\n","        \"\"\"\n","        albert = cls()\n","        if \"farm_lm_name\" in kwargs:\n","            albert.name = kwargs[\"farm_lm_name\"]\n","        else:\n","            albert.name = pretrained_model_name_or_path\n","        # We need to differentiate between loading model using FARM format and Pytorch-Transformers format\n","        farm_lm_config = Path(pretrained_model_name_or_path) / \"language_model_config.json\"\n","        if os.path.exists(farm_lm_config):\n","            # FARM style\n","            config = AlbertConfig.from_pretrained(farm_lm_config)\n","            farm_lm_model = Path(pretrained_model_name_or_path) / \"language_model.bin\"\n","            albert.model = AlbertModel.from_pretrained(farm_lm_model, config=config, **kwargs)\n","            albert.language = albert.model.config.language\n","        else:\n","            # Huggingface transformer Style\n","            albert.model = AlbertModel.from_pretrained(str(pretrained_model_name_or_path), **kwargs)\n","            albert.language = cls._get_or_infer_language_from_name(language, pretrained_model_name_or_path)\n","        return albert\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        segment_ids,\n","        padding_mask,\n","        **kwargs,\n","    ):\n","        \"\"\"\n","        Perform the forward pass of the Albert model.\n","\n","        :param input_ids: The ids of each token in the input sequence. Is a tensor of shape [batch_size, max_seq_len]\n","        :type input_ids: torch.Tensor\n","        :param segment_ids: The id of the segment. For example, in next sentence prediction, the tokens in the\n","           first sentence are marked with 0 and those in the second are marked with 1.\n","           It is a tensor of shape [batch_size, max_seq_len]\n","        :type segment_ids: torch.Tensor\n","        :param padding_mask: A mask that assigns a 1 to valid input tokens and 0 to padding tokens\n","           of shape [batch_size, max_seq_len]\n","        :return: Embeddings for each token in the input sequence.\n","\n","        \"\"\"\n","        output_tuple = self.model(\n","            input_ids,\n","            token_type_ids=segment_ids,\n","            attention_mask=padding_mask,\n","        )\n","        if self.model.encoder.config.output_hidden_states == True:\n","            sequence_output, pooled_output, all_hidden_states = output_tuple[0], output_tuple[1], output_tuple[2]\n","            return sequence_output, pooled_output, all_hidden_states\n","        else:\n","            sequence_output, pooled_output = output_tuple[0], output_tuple[1]\n","            return sequence_output, pooled_output\n","\n","    def enable_hidden_states_output(self):\n","        self.model.encoder.config.output_hidden_states = True\n","\n","    def disable_hidden_states_output(self):\n","        self.model.encoder.config.output_hidden_states = False\n","\n","\n","class Roberta(LanguageModel):\n","    \"\"\"\n","    A roberta model that wraps the HuggingFace's implementation\n","    (https://github.com/huggingface/transformers) to fit the LanguageModel class.\n","    Paper: https://arxiv.org/abs/1907.11692\n","\n","    \"\"\"\n","\n","    def __init__(self):\n","        super(Roberta, self).__init__()\n","        self.model = None\n","        self.name = \"roberta\"\n","\n","    @classmethod\n","    def load(cls, pretrained_model_name_or_path, language=None, **kwargs):\n","        \"\"\"\n","        Load a language model either by supplying\n","\n","        * the name of a remote model on s3 (\"roberta-base\" ...)\n","        * or a local path of a model trained via transformers (\"some_dir/huggingface_model\")\n","        * or a local path of a model trained via FARM (\"some_dir/farm_model\")\n","\n","        :param pretrained_model_name_or_path: name or path of a model\n","        :param language: (Optional) Name of language the model was trained for (e.g. \"german\").\n","                         If not supplied, FARM will try to infer it from the model name.\n","        :return: Language Model\n","\n","        \"\"\"\n","        roberta = cls()\n","        if \"farm_lm_name\" in kwargs:\n","            roberta.name = kwargs[\"farm_lm_name\"]\n","        else:\n","            roberta.name = pretrained_model_name_or_path\n","        # We need to differentiate between loading model using FARM format and Pytorch-Transformers format\n","        farm_lm_config = Path(pretrained_model_name_or_path) / \"language_model_config.json\"\n","        if os.path.exists(farm_lm_config):\n","            # FARM style\n","            config = RobertaConfig.from_pretrained(farm_lm_config)\n","            farm_lm_model = Path(pretrained_model_name_or_path) / \"language_model.bin\"\n","            roberta.model = RobertaModel.from_pretrained(farm_lm_model, config=config, **kwargs)\n","            roberta.language = roberta.model.config.language\n","        else:\n","            # Huggingface transformer Style\n","            roberta.model = RobertaModel.from_pretrained(str(pretrained_model_name_or_path), **kwargs)\n","            roberta.language = cls._get_or_infer_language_from_name(language, pretrained_model_name_or_path)\n","        return roberta\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        segment_ids,\n","        padding_mask,\n","        **kwargs,\n","    ):\n","        \"\"\"\n","        Perform the forward pass of the Roberta model.\n","\n","        :param input_ids: The ids of each token in the input sequence. Is a tensor of shape [batch_size, max_seq_len]\n","        :type input_ids: torch.Tensor\n","        :param segment_ids: The id of the segment. For example, in next sentence prediction, the tokens in the\n","           first sentence are marked with 0 and those in the second are marked with 1.\n","           It is a tensor of shape [batch_size, max_seq_len]\n","        :type segment_ids: torch.Tensor\n","        :param padding_mask: A mask that assigns a 1 to valid input tokens and 0 to padding tokens\n","           of shape [batch_size, max_seq_len]\n","        :return: Embeddings for each token in the input sequence.\n","\n","        \"\"\"\n","        output_tuple = self.model(\n","            input_ids,\n","            token_type_ids=segment_ids,\n","            attention_mask=padding_mask,\n","        )\n","        if self.model.encoder.config.output_hidden_states == True:\n","            sequence_output, pooled_output, all_hidden_states = output_tuple[0], output_tuple[1], output_tuple[2]\n","            return sequence_output, pooled_output, all_hidden_states\n","        else:\n","            sequence_output, pooled_output = output_tuple[0], output_tuple[1]\n","            return sequence_output, pooled_output\n","\n","    def enable_hidden_states_output(self):\n","        self.model.encoder.config.output_hidden_states = True\n","\n","    def disable_hidden_states_output(self):\n","        self.model.encoder.config.output_hidden_states = False\n","\n","\n","class XLMRoberta(LanguageModel):\n","    \"\"\"\n","    A roberta model that wraps the HuggingFace's implementation\n","    (https://github.com/huggingface/transformers) to fit the LanguageModel class.\n","    Paper: https://arxiv.org/abs/1907.11692\n","\n","    \"\"\"\n","\n","    def __init__(self):\n","        super(XLMRoberta, self).__init__()\n","        self.model = None\n","        self.name = \"xlm_roberta\"\n","\n","    @classmethod\n","    def load(cls, pretrained_model_name_or_path, language=None, **kwargs):\n","        \"\"\"\n","        Load a language model either by supplying\n","\n","        * the name of a remote model on s3 (\"xlm-roberta-base\" ...)\n","        * or a local path of a model trained via transformers (\"some_dir/huggingface_model\")\n","        * or a local path of a model trained via FARM (\"some_dir/farm_model\")\n","\n","        :param pretrained_model_name_or_path: name or path of a model\n","        :param language: (Optional) Name of language the model was trained for (e.g. \"german\").\n","                         If not supplied, FARM will try to infer it from the model name.\n","        :return: Language Model\n","\n","        \"\"\"\n","        xlm_roberta = cls()\n","        if \"farm_lm_name\" in kwargs:\n","            xlm_roberta.name = kwargs[\"farm_lm_name\"]\n","        else:\n","            xlm_roberta.name = pretrained_model_name_or_path\n","        # We need to differentiate between loading model using FARM format and Pytorch-Transformers format\n","        farm_lm_config = Path(pretrained_model_name_or_path) / \"language_model_config.json\"\n","        if os.path.exists(farm_lm_config):\n","            # FARM style\n","            config = XLMRobertaConfig.from_pretrained(farm_lm_config)\n","            farm_lm_model = Path(pretrained_model_name_or_path) / \"language_model.bin\"\n","            xlm_roberta.model = XLMRobertaModel.from_pretrained(farm_lm_model, config=config, **kwargs)\n","            xlm_roberta.language = xlm_roberta.model.config.language\n","        else:\n","            # Huggingface transformer Style\n","            xlm_roberta.model = XLMRobertaModel.from_pretrained(str(pretrained_model_name_or_path), **kwargs)\n","            xlm_roberta.language = cls._get_or_infer_language_from_name(language, pretrained_model_name_or_path)\n","        return xlm_roberta\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        segment_ids,\n","        padding_mask,\n","        **kwargs,\n","    ):\n","        \"\"\"\n","        Perform the forward pass of the XLMRoberta model.\n","\n","        :param input_ids: The ids of each token in the input sequence. Is a tensor of shape [batch_size, max_seq_len]\n","        :type input_ids: torch.Tensor\n","        :param segment_ids: The id of the segment. For example, in next sentence prediction, the tokens in the\n","           first sentence are marked with 0 and those in the second are marked with 1.\n","           It is a tensor of shape [batch_size, max_seq_len]\n","        :type segment_ids: torch.Tensor\n","        :param padding_mask: A mask that assigns a 1 to valid input tokens and 0 to padding tokens\n","           of shape [batch_size, max_seq_len]\n","        :return: Embeddings for each token in the input sequence.\n","\n","        \"\"\"\n","        output_tuple = self.model(\n","            input_ids,\n","            token_type_ids=segment_ids,\n","            attention_mask=padding_mask,\n","        )\n","        if self.model.encoder.config.output_hidden_states == True:\n","            sequence_output, pooled_output, all_hidden_states = output_tuple[0], output_tuple[1], output_tuple[2]\n","            return sequence_output, pooled_output, all_hidden_states\n","        else:\n","            sequence_output, pooled_output = output_tuple[0], output_tuple[1]\n","            return sequence_output, pooled_output\n","\n","    def enable_hidden_states_output(self):\n","        self.model.encoder.config.output_hidden_states = True\n","\n","    def disable_hidden_states_output(self):\n","        self.model.encoder.config.output_hidden_states = False\n","\n","\n","class DistilBert(LanguageModel):\n","    \"\"\"\n","    A DistilBERT model that wraps HuggingFace's implementation\n","    (https://github.com/huggingface/transformers) to fit the LanguageModel class.\n","\n","    NOTE:\n","    - DistilBert doesnt have token_type_ids, you dont need to indicate which\n","    token belongs to which segment. Just separate your segments with the separation\n","    token tokenizer.sep_token (or [SEP])\n","    - Unlike the other BERT variants, DistilBert does not output the\n","    pooled_output. An additional pooler is initialized.\n","\n","    \"\"\"\n","\n","    def __init__(self):\n","        super(DistilBert, self).__init__()\n","        self.model = None\n","        self.name = \"distilbert\"\n","        self.pooler = None\n","\n","    @classmethod\n","    def load(cls, pretrained_model_name_or_path, language=None, **kwargs):\n","        \"\"\"\n","        Load a pretrained model by supplying\n","\n","        * the name of a remote model on s3 (\"distilbert-base-german-cased\" ...)\n","        * OR a local path of a model trained via transformers (\"some_dir/huggingface_model\")\n","        * OR a local path of a model trained via FARM (\"some_dir/farm_model\")\n","\n","        :param pretrained_model_name_or_path: The path of the saved pretrained model or its name.\n","        :type pretrained_model_name_or_path: str\n","\n","        \"\"\"\n","\n","        distilbert = cls()\n","        if \"farm_lm_name\" in kwargs:\n","            distilbert.name = kwargs[\"farm_lm_name\"]\n","        else:\n","            distilbert.name = pretrained_model_name_or_path\n","        # We need to differentiate between loading model using FARM format and Pytorch-Transformers format\n","        farm_lm_config = Path(pretrained_model_name_or_path) / \"language_model_config.json\"\n","        if os.path.exists(farm_lm_config):\n","            # FARM style\n","            config = DistilBertConfig.from_pretrained(farm_lm_config)\n","            farm_lm_model = Path(pretrained_model_name_or_path) / \"language_model.bin\"\n","            distilbert.model = DistilBertModel.from_pretrained(farm_lm_model, config=config, **kwargs)\n","            distilbert.language = distilbert.model.config.language\n","        else:\n","            # Pytorch-transformer Style\n","            distilbert.model = DistilBertModel.from_pretrained(str(pretrained_model_name_or_path), **kwargs)\n","            distilbert.language = cls._get_or_infer_language_from_name(language, pretrained_model_name_or_path)\n","        config = distilbert.model.config\n","\n","        # DistilBERT does not provide a pooled_output by default. Therefore, we need to initialize an extra pooler.\n","        # The pooler takes the first hidden representation & feeds it to a dense layer of (hidden_dim x hidden_dim).\n","        # We don't want a dropout in the end of the pooler, since we do that already in the adaptive model before we\n","        # feed everything to the prediction head\n","        config.summary_last_dropout = 0\n","        config.summary_type = 'first'\n","        config.summary_activation = 'tanh'\n","        distilbert.pooler = SequenceSummary(config)\n","        distilbert.pooler.apply(distilbert.model._init_weights)\n","        return distilbert\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        padding_mask,\n","        **kwargs,\n","    ):\n","        \"\"\"\n","        Perform the forward pass of the DistilBERT model.\n","\n","        :param input_ids: The ids of each token in the input sequence. Is a tensor of shape [batch_size, max_seq_len]\n","        :type input_ids: torch.Tensor\n","        :param padding_mask: A mask that assigns a 1 to valid input tokens and 0 to padding tokens\n","           of shape [batch_size, max_seq_len]\n","        :return: Embeddings for each token in the input sequence.\n","\n","        \"\"\"\n","        output_tuple = self.model(\n","            input_ids,\n","            attention_mask=padding_mask,\n","        )\n","        # We need to manually aggregate that to get a pooled output (one vec per seq)\n","        pooled_output = self.pooler(output_tuple[0])\n","        if self.model.config.output_hidden_states == True:\n","            sequence_output, all_hidden_states = output_tuple[0], output_tuple[1]\n","            return sequence_output, pooled_output\n","        else:\n","            sequence_output = output_tuple[0]\n","            return sequence_output, pooled_output\n","\n","    def enable_hidden_states_output(self):\n","        self.model.config.output_hidden_states = True\n","\n","    def disable_hidden_states_output(self):\n","        self.model.config.output_hidden_states = False\n","\n","\n","class XLNet(LanguageModel):\n","    \"\"\"\n","    A XLNet model that wraps the HuggingFace's implementation\n","    (https://github.com/huggingface/transformers) to fit the LanguageModel class.\n","    Paper: https://arxiv.org/abs/1906.08237\n","    \"\"\"\n","\n","    def __init__(self):\n","        super(XLNet, self).__init__()\n","        self.model = None\n","        self.name = \"xlnet\"\n","        self.pooler = None\n","\n","    @classmethod\n","    def load(cls, pretrained_model_name_or_path, language=None, **kwargs):\n","        \"\"\"\n","        Load a language model either by supplying\n","\n","        * the name of a remote model on s3 (\"xlnet-base-cased\" ...)\n","        * or a local path of a model trained via transformers (\"some_dir/huggingface_model\")\n","        * or a local path of a model trained via FARM (\"some_dir/farm_model\")\n","\n","        :param pretrained_model_name_or_path: name or path of a model\n","        :param language: (Optional) Name of language the model was trained for (e.g. \"german\").\n","                         If not supplied, FARM will try to infer it from the model name.\n","        :return: Language Model\n","\n","        \"\"\"\n","        xlnet = cls()\n","        if \"farm_lm_name\" in kwargs:\n","            xlnet.name = kwargs[\"farm_lm_name\"]\n","        else:\n","            xlnet.name = pretrained_model_name_or_path\n","        # We need to differentiate between loading model using FARM format and Pytorch-Transformers format\n","        farm_lm_config = Path(pretrained_model_name_or_path) / \"language_model_config.json\"\n","        if os.path.exists(farm_lm_config):\n","            # FARM style\n","            config = XLNetConfig.from_pretrained(farm_lm_config)\n","            farm_lm_model = Path(pretrained_model_name_or_path) / \"language_model.bin\"\n","            xlnet.model = XLNetModel.from_pretrained(farm_lm_model, config=config, **kwargs)\n","            xlnet.language = xlnet.model.config.language\n","        else:\n","            # Pytorch-transformer Style\n","            xlnet.model = XLNetModel.from_pretrained(str(pretrained_model_name_or_path), **kwargs)\n","            xlnet.language = cls._get_or_infer_language_from_name(language, pretrained_model_name_or_path)\n","            config = xlnet.model.config\n","        # XLNet does not provide a pooled_output by default. Therefore, we need to initialize an extra pooler.\n","        # The pooler takes the last hidden representation & feeds it to a dense layer of (hidden_dim x hidden_dim).\n","        # We don't want a dropout in the end of the pooler, since we do that already in the adaptive model before we\n","        # feed everything to the prediction head\n","        config.summary_last_dropout = 0\n","        xlnet.pooler = SequenceSummary(config)\n","        xlnet.pooler.apply(xlnet.model._init_weights)\n","        return xlnet\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        segment_ids,\n","        padding_mask,\n","        **kwargs,\n","    ):\n","        \"\"\"\n","        Perform the forward pass of the XLNet model.\n","\n","        :param input_ids: The ids of each token in the input sequence. Is a tensor of shape [batch_size, max_seq_len]\n","        :type input_ids: torch.Tensor\n","        :param segment_ids: The id of the segment. For example, in next sentence prediction, the tokens in the\n","           first sentence are marked with 0 and those in the second are marked with 1.\n","           It is a tensor of shape [batch_size, max_seq_len]\n","        :type segment_ids: torch.Tensor\n","        :param padding_mask: A mask that assigns a 1 to valid input tokens and 0 to padding tokens\n","           of shape [batch_size, max_seq_len]\n","        :return: Embeddings for each token in the input sequence.\n","        \"\"\"\n","\n","        # Note: XLNet has a couple of special input tensors for pretraining / text generation  (perm_mask, target_mapping ...)\n","        # We will need to implement them, if we wanna support LM adaptation\n","\n","        output_tuple = self.model(\n","            input_ids,\n","            token_type_ids=segment_ids,\n","            attention_mask=padding_mask,\n","        )\n","        # XLNet also only returns the sequence_output (one vec per token)\n","        # We need to manually aggregate that to get a pooled output (one vec per seq)\n","        # TODO verify that this is really doing correct pooling\n","        pooled_output = self.pooler(output_tuple[0])\n","\n","        if self.model.output_hidden_states == True:\n","            sequence_output, all_hidden_states = output_tuple[0], output_tuple[1]\n","            return sequence_output, pooled_output, all_hidden_states\n","        else:\n","            sequence_output = output_tuple[0]\n","            return sequence_output, pooled_output\n","\n","    def enable_hidden_states_output(self):\n","        self.model.output_hidden_states = True\n","\n","    def disable_hidden_states_output(self):\n","        self.model.output_hidden_states = False\n","\n","class EmbeddingConfig():\n","    \"\"\"\n","    Config for Word Embeddings Models.\n","    Necessary to work with Bert and other LM style functionality\n","    \"\"\"\n","    def __init__(self,\n","                 name=None,\n","                 embeddings_filename=None,\n","                 vocab_filename=None,\n","                 vocab_size=None,\n","                 hidden_size=None,\n","                 language=None,\n","                 **kwargs):\n","        \"\"\"\n","        :param name: Name of config\n","        :param embeddings_filename:\n","        :param vocab_filename:\n","        :param vocab_size:\n","        :param hidden_size:\n","        :param language:\n","        :param kwargs:\n","        \"\"\"\n","        self.name = name\n","        self.embeddings_filename = embeddings_filename\n","        self.vocab_filename = vocab_filename\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","        self.language = language\n","        if len(kwargs) > 0:\n","            logger.info(f\"Passed unused params {str(kwargs)} to the EmbeddingConfig. Might not be a problem.\")\n","\n","    def to_dict(self):\n","        \"\"\"\n","        Serializes this instance to a Python dictionary.\n","\n","        Returns:\n","            :obj:`Dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,\n","        \"\"\"\n","        output = copy.deepcopy(self.__dict__)\n","        if hasattr(self.__class__, \"model_type\"):\n","            output[\"model_type\"] = self.__class__.model_type\n","        return output\n","\n","    def to_json_string(self):\n","        \"\"\"\n","        Serializes this instance to a JSON string.\n","\n","        Returns:\n","            :obj:`string`: String containing all the attributes that make up this configuration instance in JSON format.\n","        \"\"\"\n","        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n","\n","\n","\n","class EmbeddingModel():\n","    \"\"\"\n","    Embedding Model that combines\n","    - Embeddings\n","    - Config Object\n","    - Vocab\n","    Necessary to work with Bert and other LM style functionality\n","    \"\"\"\n","\n","    def __init__(self,\n","                 embedding_file,\n","                 config_dict,\n","                 vocab_file):\n","        \"\"\"\n","\n","        :param embedding_file: filename of embeddings. Usually in txt format, with the word and associated vector on each line\n","        :type embedding_file: str\n","        :param config_dict: dictionary containing config elements\n","        :type config_dict: dict\n","        :param vocab_file: filename of vocab, each line contains a word\n","        :type vocab_file: str\n","        \"\"\"\n","        self.config = EmbeddingConfig(**config_dict)\n","        self.vocab = load_vocab(vocab_file)\n","        temp = wordembedding_utils.load_embedding_vectors(embedding_file=embedding_file, vocab=self.vocab)\n","        self.embeddings = torch.from_numpy(temp).float()\n","        assert \"[UNK]\" in self.vocab, \"No [UNK] symbol in Wordembeddingmodel! Aborting\"\n","        self.unk_idx = self.vocab[\"[UNK]\"]\n","\n","    def save(self,save_dir):\n","        # Save Weights\n","        save_name = Path(save_dir) / self.config.embeddings_filename\n","        embeddings = self.embeddings.cpu().numpy()\n","        with open(save_name, \"w\") as f:\n","            for w, vec in tqdm(zip(self.vocab, embeddings), desc=\"Saving embeddings\", total=embeddings.shape[0]):\n","                f.write(w + \" \" + \" \".join([\"%.6f\" % v for v in vec]) + \"\\n\")\n","        f.close()\n","\n","        # Save vocab\n","        save_name = Path(save_dir) / self.config.vocab_filename\n","        with open(save_name, \"w\") as f:\n","            for w in self.vocab:\n","                f.write(w + \"\\n\")\n","        f.close()\n","\n","\n","    def resize_token_embeddings(self, new_num_tokens=None):\n","        # function is called as a vocab length validation inside FARM\n","        # fast way of returning an object with num_embeddings attribute (needed for some checks)\n","        # TODO add functionality to add words/tokens to a wordembeddingmodel after initialization\n","        temp = {}\n","        temp[\"num_embeddings\"] = len(self.vocab)\n","        temp = DotMap(temp)\n","        return temp\n","\n","\n","\n","class WordEmbedding_LM(LanguageModel):\n","    \"\"\"\n","    A Language Model based only on word embeddings\n","    - Inside FARM, WordEmbedding Language Models must have a fixed vocabulary\n","    - Each (known) word in some text input is projected to its vector representation\n","    - Pooling operations can be applied for representing whole text sequences\n","\n","    \"\"\"\n","\n","    def __init__(self):\n","        super(WordEmbedding_LM, self).__init__()\n","        self.model = None\n","        self.name = \"WordEmbedding_LM\"\n","        self.pooler = None\n","\n","\n","    @classmethod\n","    def load(cls, pretrained_model_name_or_path, language=None, **kwargs):\n","        \"\"\"\n","        Load a language model either by supplying\n","\n","        * a local path of a model trained via FARM (\"some_dir/farm_model\")\n","        * the name of a remote model on s3\n","\n","        :param pretrained_model_name_or_path: name or path of a model\n","        :param language: (Optional) Name of language the model was trained for (e.g. \"german\").\n","                         If not supplied, FARM will try to infer it from the model name.\n","        :return: Language Model\n","\n","        \"\"\"\n","        wordembedding_LM = cls()\n","        if \"farm_lm_name\" in kwargs:\n","            wordembedding_LM.name = kwargs[\"farm_lm_name\"]\n","        else:\n","            wordembedding_LM.name = pretrained_model_name_or_path\n","        # We need to differentiate between loading model from local or remote\n","        farm_lm_config = Path(pretrained_model_name_or_path) / \"language_model_config.json\"\n","        if os.path.exists(farm_lm_config):\n","            # local dir\n","            config = json.load(open(farm_lm_config,\"r\"))\n","            farm_lm_model = Path(pretrained_model_name_or_path) / config[\"embeddings_filename\"]\n","            vocab_filename = Path(pretrained_model_name_or_path) / config[\"vocab_filename\"]\n","            wordembedding_LM.model = EmbeddingModel(embedding_file=str(farm_lm_model), config_dict=config, vocab_file=str(vocab_filename))\n","            wordembedding_LM.language = config.get(\"language\", None)\n","        else:\n","            # from remote or cache\n","            config_dict, resolved_vocab_file, resolved_model_file = wordembedding_utils.load_model(pretrained_model_name_or_path, **kwargs)\n","            model = EmbeddingModel(embedding_file=resolved_model_file,\n","                                   config_dict=config_dict,\n","                                   vocab_file=resolved_vocab_file)\n","            wordembedding_LM.model = model\n","            wordembedding_LM.language = model.config.language\n","\n","\n","        # taking the mean for getting the pooled representation\n","        # TODO: extend this to other pooling operations or remove\n","        wordembedding_LM.pooler = lambda x: torch.mean(x, dim=0)\n","        return wordembedding_LM\n","\n","    def save(self, save_dir):\n","        \"\"\"\n","        Save the model embeddings and its config file so that it can be loaded again.\n","        # TODO make embeddings trainable and save trained embeddings\n","        # TODO save model weights as pytorch model bin for more efficient loading and saving\n","        :param save_dir: The directory in which the model should be saved.\n","        :type save_dir: str\n","        \"\"\"\n","        #save model\n","        self.model.save(save_dir=save_dir)\n","        #save config\n","        self.save_config(save_dir=save_dir)\n","\n","\n","    def forward(self, input_ids, **kwargs,):\n","        \"\"\"\n","        Perform the forward pass of the wordembedding model.\n","        This is just the mapping of words to their corresponding embeddings\n","        \"\"\"\n","        sequence_output = []\n","        pooled_output = []\n","        # TODO do not use padding items in pooled output\n","        for sample in input_ids:\n","            sample_embeddings = []\n","            for index in sample:\n","                #if index != self.model.unk_idx:\n","                sample_embeddings.append(self.model.embeddings[index])\n","            sample_embeddings = torch.stack(sample_embeddings)\n","            sequence_output.append(sample_embeddings)\n","            pooled_output.append(self.pooler(sample_embeddings))\n","\n","        sequence_output = torch.stack(sequence_output)\n","        pooled_output = torch.stack(pooled_output)\n","        m = nn.BatchNorm1d(pooled_output.shape[1])\n","        # use batchnorm for more stable learning\n","        # but disable it, if we have batch size of one (cannot compute batchnorm stats with only one sample)\n","        if pooled_output.shape[0] > 1:\n","            pooled_output = m(pooled_output)\n","        return sequence_output, pooled_output\n","\n","    def trim_vocab(self, token_counts, processor, min_threshold):\n","        \"\"\" Remove embeddings for rare tokens in your corpus (< `min_threshold` occurrences) to reduce model size\"\"\"\n","        logger.info(f\"Removing tokens with less than {min_threshold} occurrences from model vocab\")\n","        new_vocab = OrderedDict()\n","        valid_tok_indices = []\n","        cnt = 0\n","        old_num_emb = self.model.embeddings.shape[0]\n","        for token, tok_idx in self.model.vocab.items():\n","            if token_counts.get(token, 0) >= min_threshold or token in (\"[CLS]\",\"[SEP]\",\"[UNK]\",\"[PAD]\",\"[MASK]\"):\n","                new_vocab[token] = cnt\n","                valid_tok_indices.append(tok_idx)\n","                cnt += 1\n","\n","        self.model.vocab = new_vocab\n","        self.model.embeddings = self.model.embeddings[valid_tok_indices, :]\n","\n","        # update tokenizer vocab in place\n","        processor.tokenizer.vocab = self.model.vocab\n","        processor.tokenizer.ids_to_tokens = OrderedDict()\n","        for k, v in processor.tokenizer.vocab.items():\n","            processor.tokenizer.ids_to_tokens[v] = k\n","\n","        logger.info(f\"Reduced vocab from {old_num_emb} to {self.model.embeddings.shape[0]}\")\n","\n","    def normalize_embeddings(self, zero_mean=True, pca_removal=False, pca_n_components=300, pca_n_top_components=10,\n","                             use_mean_vec_for_special_tokens=True, n_special_tokens=5):\n","        \"\"\" Normalize word embeddings as in https://arxiv.org/pdf/1808.06305.pdf\n","            (e.g. used for S3E Pooling of sentence embeddings)\n","            \n","        :param zero_mean: Whether to center embeddings via subtracting mean\n","        :type zero_mean: bool\n","        :param pca_removal: Whether to remove PCA components\n","        :type pca_removal: bool\n","        :param pca_n_components: Number of PCA components to use for fitting\n","        :type pca_n_components: int\n","        :param pca_n_top_components: Number of PCA components to remove\n","        :type pca_n_top_components: int\n","        :param use_mean_vec_for_special_tokens: Whether to replace embedding of special tokens with the mean embedding\n","        :type use_mean_vec_for_special_tokens: bool\n","        :param n_special_tokens: Number of special tokens like CLS, UNK etc. (used if `use_mean_vec_for_special_tokens`). \n","                                 Note: We expect the special tokens to be the first `n_special_tokens` entries of the vocab.\n","        :type n_special_tokens: int\n","        :return: None\n","        \"\"\"\n","\n","        if zero_mean:\n","            logger.info('Removing mean from embeddings')\n","            # self.model.embeddings[:n_special_tokens, :] = torch.zeros((n_special_tokens, 300))\n","            mean_vec = torch.mean(self.model.embeddings, 0)\n","            self.model.embeddings = self.model.embeddings - mean_vec\n","\n","            if use_mean_vec_for_special_tokens:\n","                self.model.embeddings[:n_special_tokens, :] = mean_vec\n","\n","        if pca_removal:\n","            from sklearn.decomposition import PCA\n","            logger.info('Removing projections on top PCA components from embeddings (see https://arxiv.org/pdf/1808.06305.pdf)')\n","            pca = PCA(n_components=pca_n_components)\n","            pca.fit(self.model.embeddings.cpu().numpy())\n","\n","            U1 = pca.components_\n","            explained_variance = pca.explained_variance_\n","\n","            # Removing projections on top components\n","            PVN_dims = pca_n_top_components\n","            for emb_idx in tqdm(range(self.model.embeddings.shape[0]), desc=\"Removing projections\"):\n","                for pca_idx, u in enumerate(U1[0:PVN_dims]):\n","                    ratio = (explained_variance[pca_idx] - explained_variance[PVN_dims]) / explained_variance[pca_idx]\n","                    self.model.embeddings[emb_idx] = self.model.embeddings[emb_idx] - ratio * np.dot(u.transpose(), self.model.embeddings[emb_idx]) * u\n","\n","\n","class Electra(LanguageModel):\n","    \"\"\"\n","    ELECTRA is a new pre-training approach which trains two transformer models:\n","    the generator and the discriminator. The generator replaces tokens in a sequence,\n","    and is therefore trained as a masked language model. The discriminator, which is\n","    the model we're interested in, tries to identify which tokens were replaced by\n","    the generator in the sequence.\n","\n","    The ELECTRA model here wraps HuggingFace's implementation\n","    (https://github.com/huggingface/transformers) to fit the LanguageModel class.\n","\n","    NOTE:\n","    - Electra does not output the pooled_output. An additional pooler is initialized.\n","\n","    \"\"\"\n","\n","    def __init__(self):\n","        super(Electra, self).__init__()\n","        self.model = None\n","        self.name = \"electra\"\n","        self.pooler = None\n","\n","    @classmethod\n","    def load(cls, pretrained_model_name_or_path, language=None, **kwargs):\n","        \"\"\"\n","        Load a pretrained model by supplying\n","\n","        * the name of a remote model on s3 (\"google/electra-base-discriminator\" ...)\n","        * OR a local path of a model trained via transformers (\"some_dir/huggingface_model\")\n","        * OR a local path of a model trained via FARM (\"some_dir/farm_model\")\n","\n","        :param pretrained_model_name_or_path: The path of the saved pretrained model or its name.\n","        :type pretrained_model_name_or_path: str\n","\n","        \"\"\"\n","\n","        electra = cls()\n","        if \"farm_lm_name\" in kwargs:\n","            electra.name = kwargs[\"farm_lm_name\"]\n","        else:\n","            electra.name = pretrained_model_name_or_path\n","        # We need to differentiate between loading model using FARM format and Transformers format\n","        farm_lm_config = Path(pretrained_model_name_or_path) / \"language_model_config.json\"\n","        if os.path.exists(farm_lm_config):\n","            # FARM style\n","            config = ElectraConfig.from_pretrained(farm_lm_config)\n","            farm_lm_model = Path(pretrained_model_name_or_path) / \"language_model.bin\"\n","            electra.model = ElectraModel.from_pretrained(farm_lm_model, config=config, **kwargs)\n","            electra.language = electra.model.config.language\n","        else:\n","            # Transformers Style\n","            electra.model = ElectraModel.from_pretrained(str(pretrained_model_name_or_path), **kwargs)\n","            electra.language = cls._get_or_infer_language_from_name(language, pretrained_model_name_or_path)\n","        config = electra.model.config\n","\n","        # ELECTRA does not provide a pooled_output by default. Therefore, we need to initialize an extra pooler.\n","        # The pooler takes the first hidden representation & feeds it to a dense layer of (hidden_dim x hidden_dim).\n","        # We don't want a dropout in the end of the pooler, since we do that already in the adaptive model before we\n","        # feed everything to the prediction head.\n","        # Note: ELECTRA uses gelu as activation (BERT uses tanh instead)\n","        config.summary_last_dropout = 0\n","        config.summary_type = 'first'\n","        config.summary_activation = 'gelu'\n","        config.summary_use_proj = False\n","        electra.pooler = SequenceSummary(config)\n","        electra.pooler.apply(electra.model._init_weights)\n","        return electra\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        segment_ids,\n","        padding_mask,\n","        **kwargs,\n","    ):\n","        \"\"\"\n","        Perform the forward pass of the ELECTRA model.\n","\n","        :param input_ids: The ids of each token in the input sequence. Is a tensor of shape [batch_size, max_seq_len]\n","        :type input_ids: torch.Tensor\n","        :param padding_mask: A mask that assigns a 1 to valid input tokens and 0 to padding tokens\n","           of shape [batch_size, max_seq_len]\n","        :return: Embeddings for each token in the input sequence.\n","\n","        \"\"\"\n","        output_tuple = self.model(\n","            input_ids,\n","            token_type_ids=segment_ids,\n","            attention_mask=padding_mask,\n","        )\n","\n","        # We need to manually aggregate that to get a pooled output (one vec per seq)\n","        pooled_output = self.pooler(output_tuple[0])\n","\n","        if self.model.config.output_hidden_states == True:\n","            sequence_output, all_hidden_states = output_tuple[0], output_tuple[1]\n","            return sequence_output, pooled_output\n","        else:\n","            sequence_output = output_tuple[0]\n","            return sequence_output, pooled_output\n","\n","    def enable_hidden_states_output(self):\n","        self.model.config.output_hidden_states = True\n","\n","    def disable_hidden_states_output(self):\n","        self.model.config.output_hidden_states = False\n","\n","\n","class Camembert(Roberta):\n","    \"\"\"\n","    A Camembert model that wraps the HuggingFace's implementation\n","    (https://github.com/huggingface/transformers) to fit the LanguageModel class.\n","    \"\"\"\n","    def __init__(self):\n","        super(Camembert, self).__init__()\n","        self.model = None\n","        self.name = \"camembert\"\n","\n","    @classmethod\n","    def load(cls, pretrained_model_name_or_path, language=None, **kwargs):\n","        \"\"\"\n","        Load a language model either by supplying\n","\n","        * the name of a remote model on s3 (\"camembert-base\" ...)\n","        * or a local path of a model trained via transformers (\"some_dir/huggingface_model\")\n","        * or a local path of a model trained via FARM (\"some_dir/farm_model\")\n","\n","        :param pretrained_model_name_or_path: name or path of a model\n","        :param language: (Optional) Name of language the model was trained for (e.g. \"german\").\n","                         If not supplied, FARM will try to infer it from the model name.\n","        :return: Language Model\n","\n","        \"\"\"\n","        camembert = cls()\n","        if \"farm_lm_name\" in kwargs:\n","            camembert.name = kwargs[\"farm_lm_name\"]\n","        else:\n","            camembert.name = pretrained_model_name_or_path\n","        # We need to differentiate between loading model using FARM format and Pytorch-Transformers format\n","        farm_lm_config = Path(pretrained_model_name_or_path) / \"language_model_config.json\"\n","        if os.path.exists(farm_lm_config):\n","            # FARM style\n","            config = CamembertConfig.from_pretrained(farm_lm_config)\n","            farm_lm_model = Path(pretrained_model_name_or_path) / \"language_model.bin\"\n","            camembert.model = CamembertModel.from_pretrained(farm_lm_model, config=config, **kwargs)\n","            camembert.language = camembert.model.config.language\n","        else:\n","            # Huggingface transformer Style\n","            camembert.model = CamembertModel.from_pretrained(str(pretrained_model_name_or_path), **kwargs)\n","            camembert.language = cls._get_or_infer_language_from_name(language, pretrained_model_name_or_path)\n","        return camembert\n","\n","\n","class DPRQuestionEncoder(LanguageModel):\n","    \"\"\"\n","    A DPRQuestionEncoder model that wraps HuggingFace's implementation\n","    \"\"\"\n","\n","    def __init__(self):\n","        super(DPRQuestionEncoder, self).__init__()\n","        self.model = None\n","        self.name = \"dpr_question_encoder\"\n","\n","    @classmethod\n","    def load(cls, pretrained_model_name_or_path, language=None, **kwargs):\n","        \"\"\"\n","        Load a pretrained model by supplying\n","\n","        * the name of a remote model on s3 (\"facebook/dpr-question_encoder-single-nq-base\" ...)\n","        * OR a local path of a model trained via transformers (\"some_dir/huggingface_model\")\n","        * OR a local path of a model trained via FARM (\"some_dir/farm_model\")\n","\n","        :param pretrained_model_name_or_path: The path of the base pretrained language model whose weights are used to initialize DPRQuestionEncoder\n","        :type pretrained_model_name_or_path: str\n","        \"\"\"\n","\n","        dpr_question_encoder = cls()\n","        if \"farm_lm_name\" in kwargs:\n","            dpr_question_encoder.name = kwargs[\"farm_lm_name\"]\n","        else:\n","            dpr_question_encoder.name = pretrained_model_name_or_path\n","\n","        # We need to differentiate between loading model using FARM format and Pytorch-Transformers format\n","        farm_lm_config = Path(pretrained_model_name_or_path) / \"language_model_config.json\"\n","        if os.path.exists(farm_lm_config):\n","            # FARM style\n","            dpr_config = transformers.DPRConfig.from_pretrained(farm_lm_config)\n","            farm_lm_model = Path(pretrained_model_name_or_path) / \"language_model.bin\"\n","            dpr_question_encoder.model = transformers.DPRQuestionEncoder(config=dpr_config, **kwargs)\n","            dpr_question_encoder.language = dpr_question_encoder.model.config.language\n","        else:\n","            original_model_config = AutoConfig.from_pretrained(pretrained_model_name_or_path)\n","            if original_model_config.model_type == \"dpr\":\n","                # \"pretrained dpr model\": load existing pretrained DPRQuestionEncoder model\n","                dpr_question_encoder.model = transformers.DPRQuestionEncoder(config=dpr_config, **kwargs)\n","            else:\n","                # \"from scratch\": load weights from different architecture (e.g. bert) into DPRQuestionEncoder\n","                # but keep config values from original architecture\n","                # TODO test for architectures other than BERT, e.g. Electra\n","                if original_model_config.model_type != \"bert\":\n","                    logger.warning(f\"Using a model of type '{original_model_config.model_type}' which might be incompatible with DPR encoders.\"\n","                                   f\"Bert based encoders are supported that need input_ids,token_type_ids,attention_mask as input tensors.\")\n","                original_config_dict = vars(original_model_config)\n","                original_config_dict.update(kwargs)\n","                dpr_question_encoder.model = transformers.DPRQuestionEncoder(config=transformers.DPRConfig(**original_config_dict))\n","                dpr_question_encoder.model.base_model.bert_model = AutoModel.from_pretrained(\n","                    str(pretrained_model_name_or_path), **original_config_dict)\n","            dpr_question_encoder.language = cls._get_or_infer_language_from_name(language, pretrained_model_name_or_path)\n","\n","        return dpr_question_encoder\n","\n","    def forward(\n","        self,\n","        query_input_ids,\n","        query_segment_ids,\n","        query_attention_mask,\n","        **kwargs,\n","    ):\n","        \"\"\"\n","        Perform the forward pass of the DPRQuestionEncoder model.\n","\n","        :param query_input_ids: The ids of each token in the input sequence. Is a tensor of shape [batch_size, max_seq_len]\n","        :type query_input_ids: torch.Tensor\n","        :param query_segment_ids: The id of the segment. For example, in next sentence prediction, the tokens in the\n","           first sentence are marked with 0 and those in the second are marked with 1.\n","           It is a tensor of shape [batch_size, max_seq_len]\n","        :type query_segment_ids: torch.Tensor\n","        :param query_attention_mask: A mask that assigns a 1 to valid input tokens and 0 to padding tokens\n","           of shape [batch_size, max_seq_len]\n","        :type query_attention_mask: torch.Tensor\n","        :return: Embeddings for each token in the input sequence.\n","\n","        \"\"\"\n","        output_tuple = self.model(\n","            input_ids=query_input_ids,\n","            token_type_ids=query_segment_ids,\n","            attention_mask=query_attention_mask,\n","            return_dict=True\n","        )\n","        if self.model.question_encoder.config.output_hidden_states == True:\n","            pooled_output, all_hidden_states = output_tuple.pooler_output, output_tuple.hidden_states\n","            return pooled_output, all_hidden_states\n","        else:\n","            pooled_output = output_tuple.pooler_output\n","            return pooled_output, None\n","\n","    def enable_hidden_states_output(self):\n","        self.model.question_encoder.config.output_hidden_states = True\n","\n","    def disable_hidden_states_output(self):\n","        self.model.question_encoder.config.output_hidden_states = False\n","\n","\n","class DPRContextEncoder(LanguageModel):\n","    \"\"\"\n","    A DPRContextEncoder model that wraps HuggingFace's implementation\n","    \"\"\"\n","\n","    def __init__(self):\n","        super(DPRContextEncoder, self).__init__()\n","        self.model = None\n","        self.name = \"dpr_context_encoder\"\n","\n","    @classmethod\n","    def load(cls, pretrained_model_name_or_path, language=None, **kwargs):\n","        \"\"\"\n","        Load a pretrained model by supplying\n","\n","        * the name of a remote model on s3 (\"facebook/dpr-ctx_encoder-single-nq-base\" ...)\n","        * OR a local path of a model trained via transformers (\"some_dir/huggingface_model\")\n","        * OR a local path of a model trained via FARM (\"some_dir/farm_model\")\n","\n","        :param pretrained_model_name_or_path: The path of the base pretrained language model whose weights are used to initialize DPRContextEncoder\n","        :type pretrained_model_name_or_path: str\n","        \"\"\"\n","\n","        dpr_context_encoder = cls()\n","        if \"farm_lm_name\" in kwargs:\n","            dpr_context_encoder.name = kwargs[\"farm_lm_name\"]\n","        else:\n","            dpr_context_encoder.name = pretrained_model_name_or_path\n","        # We need to differentiate between loading model using FARM format and Pytorch-Transformers format\n","        farm_lm_config = Path(pretrained_model_name_or_path) / \"language_model_config.json\"\n","        if os.path.exists(farm_lm_config):\n","            # FARM style\n","            dpr_config = transformers.DPRConfig.from_pretrained(farm_lm_config)\n","            farm_lm_model = Path(pretrained_model_name_or_path) / \"language_model.bin\"\n","            dpr_context_encoder.model = transformers.DPRContextEncoder(config=dpr_config, **kwargs)\n","            dpr_context_encoder.language = dpr_context_encoder.model.config.language\n","        else:\n","            # Pytorch-transformer Style\n","            original_model_config = AutoConfig.from_pretrained(pretrained_model_name_or_path)\n","            if original_model_config.model_type == \"dpr\":\n","                # \"pretrained dpr model\": load existing pretrained DPRContextEncoder model\n","                dpr_context_encoder.model = transformers.DPRContextEncoder(config=dpr_config, **kwargs)\n","            else:\n","                # \"from scratch\": load weights from different architecture (e.g. bert) into DPRContextEncoder\n","                # but keep config values from original architecture\n","                # TODO test for architectures other than BERT, e.g. Electra\n","                if original_model_config.model_type != \"bert\":\n","                    logger.warning(\n","                        f\"Using a model of type '{original_model_config.model_type}' which might be incompatible with DPR encoders.\"\n","                        f\"Bert based encoders are supported that need input_ids,token_type_ids,attention_mask as input tensors.\")\n","                original_config_dict = vars(original_model_config)\n","                original_config_dict.update(kwargs)\n","                dpr_context_encoder.model = transformers.DPRContextEncoder(\n","                    config=transformers.DPRConfig(**original_config_dict))\n","                dpr_context_encoder.model.base_model.bert_model = AutoModel.from_pretrained(\n","                    str(pretrained_model_name_or_path), **original_config_dict)\n","            dpr_context_encoder.language = cls._get_or_infer_language_from_name(language, pretrained_model_name_or_path)\n","\n","        return dpr_context_encoder\n","\n","    def forward(\n","        self,\n","        passage_input_ids,\n","        passage_segment_ids,\n","        passage_attention_mask,\n","        **kwargs,\n","    ):\n","        \"\"\"\n","        Perform the forward pass of the DPRContextEncoder model.\n","\n","        :param passage_input_ids: The ids of each token in the input sequence. Is a tensor of shape [batch_size, number_of_hard_negative_passages, max_seq_len]\n","        :type passage_input_ids: torch.Tensor\n","        :param passage_segment_ids: The id of the segment. For example, in next sentence prediction, the tokens in the\n","           first sentence are marked with 0 and those in the second are marked with 1.\n","           It is a tensor of shape [batch_size, number_of_hard_negative_passages, max_seq_len]\n","        :type passage_segment_ids: torch.Tensor\n","        :param passage_attention_mask: A mask that assigns a 1 to valid input tokens and 0 to padding tokens\n","           of shape [batch_size,  number_of_hard_negative_passages, max_seq_len]\n","        :return: Embeddings for each token in the input sequence.\n","\n","        \"\"\"\n","        max_seq_len = passage_input_ids.shape[-1]\n","        passage_input_ids = passage_input_ids.view(-1, max_seq_len)\n","        passage_segment_ids = passage_segment_ids.view(-1, max_seq_len)\n","        passage_attention_mask = passage_attention_mask.view(-1, max_seq_len)\n","        output_tuple = self.model(\n","            input_ids=passage_input_ids,\n","            token_type_ids=passage_segment_ids,\n","            attention_mask=passage_attention_mask,\n","            return_dict=True\n","        )\n","        if self.model.ctx_encoder.config.output_hidden_states == True:\n","            pooled_output, all_hidden_states = output_tuple.pooler_output, output_tuple.hidden_states\n","            return pooled_output, all_hidden_states\n","        else:\n","            pooled_output = output_tuple.pooler_output\n","            return pooled_output, None\n","\n","    def enable_hidden_states_output(self):\n","        self.model.ctx_encoder.config.output_hidden_states = True\n","\n","    def disable_hidden_states_output(self):\n","        self.model.ctx_encoder.config.output_hidden_states = False\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mjvaDGJAO3Sv"},"source":["## DensePassageRetriever"]},{"cell_type":"code","metadata":{"id":"O4gvWQPS8UmA"},"source":["import logging\n","from typing import List, Union, Optional\n","import torch\n","import numpy as np\n","from pathlib import Path\n","from tqdm import tqdm\n","\n","from haystack.document_store.base import BaseDocumentStore\n","from haystack import Document\n","from haystack.retriever.base import BaseRetriever\n","\n","from farm.infer import Inferencer\n","from farm.modeling.tokenization import Tokenizer\n","from farm.modeling.language_model import LanguageModel\n","from farm.modeling.biadaptive_model import BiAdaptiveModel\n","from farm.modeling.prediction_head import TextSimilarityHead\n","from farm.data_handler.processor import TextSimilarityProcessor\n","from farm.data_handler.data_silo import DataSilo\n","from farm.data_handler.dataloader import NamedDataLoader\n","from farm.modeling.optimization import initialize_optimizer\n","from farm.train import Trainer\n","from torch.utils.data.sampler import SequentialSampler\n","\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","class DensePassageRetriever(BaseRetriever):\n","    \"\"\"\n","        Retriever that uses a bi-encoder (one transformer for query, one transformer for passage).\n","        See the original paper for more details:\n","        Karpukhin, Vladimir, et al. (2020): \"Dense Passage Retrieval for Open-Domain Question Answering.\"\n","        (https://arxiv.org/abs/2004.04906).\n","    \"\"\"\n","\n","    def __init__(self,\n","                 document_store: BaseDocumentStore,\n","                 query_embedding_model: Union[Path, str] = \"voidful/dpr-question_encoder-bert-base-multilingual\",\n","                 passage_embedding_model: Union[Path, str] = \"voidful/dpr-ctx_encoder-bert-base-multilingual\",\n","                 single_model_path: Optional[Union[Path, str]] = None,\n","                 model_version: Optional[str] = None,\n","                 max_seq_len_query: int = 64,\n","                 max_seq_len_passage: int = 256,\n","                 top_k: int = 10,\n","                 use_gpu: bool = True,\n","                 batch_size: int = 16,\n","                 embed_title: bool = True,\n","                 use_fast_tokenizers: bool = True,\n","                 infer_tokenizer_classes: bool = False,\n","                 similarity_function: str = \"dot_product\",\n","                 progress_bar: bool = True\n","                 ):\n","        \"\"\"\n","        Init the Retriever incl. the two encoder models from a local or remote model checkpoint.\n","        The checkpoint format matches huggingface transformers' model format\n","\n","        **Example:**\n","\n","                ```python\n","                |    # remote model from FAIR\n","                |    DensePassageRetriever(document_store=your_doc_store,\n","                |                          query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n","                |                          passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\")\n","                |    # or from local path\n","                |    DensePassageRetriever(document_store=your_doc_store,\n","                |                          query_embedding_model=\"model_directory/question-encoder\",\n","                |                          passage_embedding_model=\"model_directory/context-encoder\")\n","                ```\n","\n","        :param document_store: An instance of DocumentStore from which to retrieve documents.\n","        :param query_embedding_model: Local path or remote name of question encoder checkpoint. The format equals the\n","                                      one used by hugging-face transformers' modelhub models\n","                                      Currently available remote names: ``\"facebook/dpr-question_encoder-single-nq-base\"``\n","        :param passage_embedding_model: Local path or remote name of passage encoder checkpoint. The format equals the\n","                                        one used by hugging-face transformers' modelhub models\n","                                        Currently available remote names: ``\"facebook/dpr-ctx_encoder-single-nq-base\"``\n","        :param single_model_path: Local path or remote name of a query and passage embedder in one single model. Those\n","                                  models are typically trained within FARM.\n","                                  Currently available remote names: TODO add FARM DPR model to HF modelhub\n","        :param model_version: The version of model to use from the HuggingFace model hub. Can be tag name, branch name, or commit hash.\n","        :param max_seq_len_query: Longest length of each query sequence. Maximum number of tokens for the query text. Longer ones will be cut down.\"\n","        :param max_seq_len_passage: Longest length of each passage/context sequence. Maximum number of tokens for the passage text. Longer ones will be cut down.\"\n","        :param top_k: How many documents to return per query.\n","        :param use_gpu: Whether to use gpu or not\n","        :param batch_size: Number of questions or passages to encode at once\n","        :param embed_title: Whether to concatenate title and passage to a text pair that is then used to create the embedding.\n","                            This is the approach used in the original paper and is likely to improve performance if your\n","                            titles contain meaningful information for retrieval (topic, entities etc.) .\n","                            The title is expected to be present in doc.meta[\"name\"] and can be supplied in the documents\n","                            before writing them to the DocumentStore like this:\n","                            {\"text\": \"my text\", \"meta\": {\"name\": \"my title\"}}.\n","        :param use_fast_tokenizers: Whether to use fast Rust tokenizers\n","        :param infer_tokenizer_classes: Whether to infer tokenizer class from the model config / name. \n","                                        If `False`, the class always loads `DPRQuestionEncoderTokenizer` and `DPRContextEncoderTokenizer`. \n","        :param similarity_function: Which function to apply for calculating the similarity of query and passage embeddings during training. \n","                                    Options: `dot_product` (Default) or `cosine`\n","        :param progress_bar: Whether to show a tqdm progress bar or not.\n","                             Can be helpful to disable in production deployments to keep the logs clean.\n","        \"\"\"\n","\n","        # save init parameters to enable export of component config as YAML\n","        self.set_config(\n","            document_store=document_store, query_embedding_model=query_embedding_model,\n","            passage_embedding_model=passage_embedding_model, single_model_path=single_model_path,\n","            model_version=model_version, max_seq_len_query=max_seq_len_query, max_seq_len_passage=max_seq_len_passage,\n","            top_k=top_k, use_gpu=use_gpu, batch_size=batch_size, embed_title=embed_title,\n","            use_fast_tokenizers=use_fast_tokenizers, infer_tokenizer_classes=infer_tokenizer_classes,\n","            similarity_function=similarity_function, progress_bar=progress_bar,\n","        )\n","\n","        self.document_store = document_store\n","        self.batch_size = batch_size\n","        self.progress_bar = progress_bar\n","        self.top_k = top_k\n","\n","        if document_store is None:\n","           logger.warning(\"DensePassageRetriever initialized without a document store. \"\n","                          \"This is fine if you are performing DPR training. \"\n","                          \"Otherwise, please provide a document store in the constructor.\")\n","        elif document_store.similarity != \"dot_product\":\n","            logger.warning(f\"You are using a Dense Passage Retriever model with the {document_store.similarity} function. \"\n","                           \"We recommend you use dot_product instead. \"\n","                           \"This can be set when initializing the DocumentStore\")\n","\n","        if use_gpu and torch.cuda.is_available():\n","            self.device = torch.device(\"cuda\")\n","        else:\n","            self.device = torch.device(\"cpu\")\n","\n","        self.infer_tokenizer_classes = infer_tokenizer_classes\n","        tokenizers_default_classes = {\n","            \"query\": \"BertTokenizer\",\n","            \"passage\": \"BertTokenizer\"\n","        }\n","        if self.infer_tokenizer_classes:\n","            tokenizers_default_classes[\"query\"] = None   # type: ignore\n","            tokenizers_default_classes[\"passage\"] = None # type: ignore\n","\n","        # Init & Load Encoders\n","        if single_model_path is None:\n","            self.query_tokenizer = Tokenizer.load(pretrained_model_name_or_path='bert-base-multilingual-cased',\n","                                                  revision=model_version,\n","                                                  do_lower_case=True,\n","                                                  use_fast=use_fast_tokenizers,\n","                                                  tokenizer_class=\"BertTokenizer\")\n","            self.query_encoder = LanguageModel.load(pretrained_model_name_or_path='voidful/dpr-question_encoder-bert-base-multilingual',\n","                                                    revision=model_version,\n","                                                    language_model_class=\"DPRQuestionEncoder\")\n","            self.passage_tokenizer = Tokenizer.load(pretrained_model_name_or_path='bert-base-multilingual-cased',\n","                                                    revision=model_version,\n","                                                    do_lower_case=True,\n","                                                    use_fast=use_fast_tokenizers,\n","                                                    tokenizer_class=\"BertTokenizer\")\n","            self.passage_encoder = LanguageModel.load(pretrained_model_name_or_path='voidful/dpr-ctx_encoder-bert-base-multilingual',\n","                                                      revision=model_version,\n","                                                      language_model_class=\"DPRContextEncoder\")\n","\n","            self.processor = TextSimilarityProcessor(query_tokenizer=self.query_tokenizer,\n","                                                     passage_tokenizer=self.passage_tokenizer,\n","                                                     max_seq_len_passage=max_seq_len_passage,\n","                                                     max_seq_len_query=max_seq_len_query,\n","                                                     label_list=[\"hard_negative\", \"positive\"],\n","                                                     metric=\"text_similarity_metric\",\n","                                                     embed_title=embed_title,\n","                                                     num_hard_negatives=0,\n","                                                     num_positives=1)\n","            prediction_head = TextSimilarityHead(similarity_function=similarity_function)\n","            self.model = BiAdaptiveModel(\n","                language_model1=self.query_encoder,\n","                language_model2=self.passage_encoder,\n","                prediction_heads=[prediction_head],\n","                embeds_dropout_prob=0.1,\n","                lm1_output_types=[\"per_sequence\"],\n","                lm2_output_types=[\"per_sequence\"],\n","                device=self.device,\n","            )\n","        else:\n","            self.processor = TextSimilarityProcessor.load_from_dir(single_model_path)\n","            self.processor.max_seq_len_passage = max_seq_len_passage\n","            self.processor.max_seq_len_query = max_seq_len_query\n","            self.processor.embed_title = embed_title\n","            self.processor.num_hard_negatives = 0\n","            self.processor.num_positives = 1  # during indexing of documents only one embedding is created\n","            self.model = BiAdaptiveModel.load(single_model_path, device=self.device)\n","\n","        self.model.connect_heads_with_processor(self.processor.tasks, require_labels=False)\n","\n","    def retrieve(self, query: str, filters: dict = None, top_k: Optional[int] = None, index: str = None) -> List[Document]:\n","        \"\"\"\n","        Scan through documents in DocumentStore and return a small number documents\n","        that are most relevant to the query.\n","\n","        :param query: The query\n","        :param filters: A dictionary where the keys specify a metadata field and the value is a list of accepted values for that field\n","        :param top_k: How many documents to return per query.\n","        :param index: The name of the index in the DocumentStore from which to retrieve documents\n","        \"\"\"\n","        if top_k is None:\n","            top_k = self.top_k\n","        if not self.document_store:\n","            logger.error(\"Cannot perform retrieve() since DensePassageRetriever initialized with document_store=None\")\n","            return []\n","        if index is None:\n","            index = self.document_store.index\n","        query_emb = self.embed_queries(texts=[query])\n","        documents = self.document_store.query_by_embedding(query_emb=query_emb[0], top_k=top_k, filters=filters, index=index)\n","        return documents\n","\n","    def _get_predictions(self, dicts):\n","        \"\"\"\n","        Feed a preprocessed dataset to the model and get the actual predictions (forward pass + formatting).\n","\n","        :param dicts: list of dictionaries\n","        examples:[{'query': \"where is florida?\"}, {'query': \"who wrote lord of the rings?\"}, ...]\n","                [{'passages': [{\n","                    \"title\": 'Big Little Lies (TV series)',\n","                    \"text\": 'series garnered several accolades. It received..',\n","                    \"label\": 'positive',\n","                    \"external_id\": '18768923'},\n","                    {\"title\": 'Framlingham Castle',\n","                    \"text\": 'Castle on the Hill \"Castle on the Hill\" is a song by English..',\n","                    \"label\": 'positive',\n","                    \"external_id\": '19930582'}, ...]\n","        :return: dictionary of embeddings for \"passages\" and \"query\"\n","        \"\"\"\n","\n","        dataset, tensor_names, _, baskets = self.processor.dataset_from_dicts(\n","            dicts, indices=[i for i in range(len(dicts))], return_baskets=True\n","        )\n","\n","        data_loader = NamedDataLoader(\n","            dataset=dataset, sampler=SequentialSampler(dataset), batch_size=self.batch_size, tensor_names=tensor_names\n","        )\n","        all_embeddings = {\"query\": [], \"passages\": []}\n","        self.model.eval()\n","\n","        # When running evaluations etc., we don't want a progress bar for every single query\n","        if len(dataset) == 1:\n","            disable_tqdm=True\n","        else:\n","            disable_tqdm = not self.progress_bar\n","\n","        for i, batch in enumerate(tqdm(data_loader, desc=f\"Creating Embeddings\", unit=\" Batches\", disable=disable_tqdm)):\n","            batch = {key: batch[key].to(self.device) for key in batch}\n","\n","            # get logits\n","            with torch.no_grad():\n","                query_embeddings, passage_embeddings = self.model.forward(**batch)[0]\n","                if query_embeddings is not None:\n","                    all_embeddings[\"query\"].append(query_embeddings.cpu().numpy())\n","                if passage_embeddings is not None:\n","                    all_embeddings[\"passages\"].append(passage_embeddings.cpu().numpy())\n","\n","        if all_embeddings[\"passages\"]:\n","            all_embeddings[\"passages\"] = np.concatenate(all_embeddings[\"passages\"])\n","        if all_embeddings[\"query\"]:\n","            all_embeddings[\"query\"] = np.concatenate(all_embeddings[\"query\"])\n","        return all_embeddings\n","\n","    def embed_queries(self, texts: List[str]) -> List[np.ndarray]:\n","        \"\"\"\n","        Create embeddings for a list of queries using the query encoder\n","\n","        :param texts: Queries to embed\n","        :return: Embeddings, one per input queries\n","        \"\"\"\n","        queries = [{'query': q} for q in texts]\n","        result = self._get_predictions(queries)[\"query\"]\n","        return result\n","\n","    def embed_passages(self, docs: List[Document]) -> List[np.ndarray]:\n","        \"\"\"\n","        Create embeddings for a list of passages using the passage encoder\n","\n","        :param docs: List of Document objects used to represent documents / passages in a standardized way within Haystack.\n","        :return: Embeddings of documents / passages shape (batch_size, embedding_dim)\n","        \"\"\"\n","        passages = [{'passages': [{\n","            \"title\": d.meta[\"name\"] if d.meta and \"name\" in d.meta else \"\",\n","            \"text\": d.text,\n","            \"label\": d.meta[\"label\"] if d.meta and \"label\" in d.meta else \"positive\",\n","            \"external_id\": d.id}]\n","        } for d in docs]\n","        embeddings = self._get_predictions(passages)[\"passages\"]\n","\n","        return embeddings\n","\n","    def train(self,\n","              data_dir: str,\n","              train_filename: str,\n","              dev_filename: str = None,\n","              test_filename: str = None,\n","              max_sample: int = None,\n","              max_processes: int = 128,\n","              dev_split: float = 0,\n","              batch_size: int = 2,\n","              embed_title: bool = True,\n","              num_hard_negatives: int = 1,\n","              num_positives: int = 1,\n","              n_epochs: int = 3,\n","              evaluate_every: int = 1000,\n","              n_gpu: int = 1,\n","              learning_rate: float = 1e-5,\n","              epsilon: float = 1e-08,\n","              weight_decay: float = 0.0,\n","              num_warmup_steps: int = 100,\n","              grad_acc_steps: int = 1,\n","              optimizer_name: str = \"TransformersAdamW\",\n","              optimizer_correct_bias: bool = True,\n","              save_dir: str = \"../saved_models/dpr\",\n","              query_encoder_save_dir: str = \"query_encoder\",\n","              passage_encoder_save_dir: str = \"passage_encoder\"\n","              ):\n","        \"\"\"\n","        train a DensePassageRetrieval model\n","        :param data_dir: Directory where training file, dev file and test file are present\n","        :param train_filename: training filename\n","        :param dev_filename: development set filename, file to be used by model in eval step of training\n","        :param test_filename: test set filename, file to be used by model in test step after training\n","        :param max_sample: maximum number of input samples to convert. Can be used for debugging a smaller dataset.\n","        :param max_processes: the maximum number of processes to spawn in the multiprocessing.Pool used in DataSilo.\n","                              It can be set to 1 to disable the use of multiprocessing or make debugging easier.\n","        :param dev_split: The proportion of the train set that will sliced. Only works if dev_filename is set to None\n","        :param batch_size: total number of samples in 1 batch of data\n","        :param embed_title: whether to concatenate passage title with each passage. The default setting in official DPR embeds passage title with the corresponding passage\n","        :param num_hard_negatives: number of hard negative passages(passages which are very similar(high score by BM25) to query but do not contain the answer\n","        :param num_positives: number of positive passages\n","        :param n_epochs: number of epochs to train the model on\n","        :param evaluate_every: number of training steps after evaluation is run\n","        :param n_gpu: number of gpus to train on\n","        :param learning_rate: learning rate of optimizer\n","        :param epsilon: epsilon parameter of optimizer\n","        :param weight_decay: weight decay parameter of optimizer\n","        :param grad_acc_steps: number of steps to accumulate gradient over before back-propagation is done\n","        :param optimizer_name: what optimizer to use (default: TransformersAdamW)\n","        :param num_warmup_steps: number of warmup steps\n","        :param optimizer_correct_bias: Whether to correct bias in optimizer\n","        :param save_dir: directory where models are saved\n","        :param query_encoder_save_dir: directory inside save_dir where query_encoder model files are saved\n","        :param passage_encoder_save_dir: directory inside save_dir where passage_encoder model files are saved\n","        \"\"\"\n","\n","        self.processor.embed_title = embed_title\n","        self.processor.data_dir = Path(data_dir)\n","        self.processor.train_filename = train_filename\n","        self.processor.dev_filename = dev_filename\n","        self.processor.test_filename = test_filename\n","        self.processor.max_sample = max_sample\n","        self.processor.dev_split = dev_split\n","        self.processor.num_hard_negatives = num_hard_negatives\n","        self.processor.num_positives = num_positives\n","\n","        self.model.connect_heads_with_processor(self.processor.tasks, require_labels=True)\n","\n","        data_silo = DataSilo(processor=self.processor, batch_size=batch_size, distributed=False, max_processes=max_processes)\n","\n","        # 5. Create an optimizer\n","        self.model, optimizer, lr_schedule = initialize_optimizer(\n","            model=self.model,\n","            learning_rate=learning_rate,\n","            optimizer_opts={\"name\": optimizer_name, \"correct_bias\": optimizer_correct_bias,\n","                            \"weight_decay\": weight_decay, \"eps\": epsilon},\n","            schedule_opts={\"name\": \"LinearWarmup\", \"num_warmup_steps\": num_warmup_steps},\n","            n_batches=len(data_silo.loaders[\"train\"]),\n","            n_epochs=n_epochs,\n","            grad_acc_steps=grad_acc_steps,\n","            device=self.device\n","        )\n","\n","        # 6. Feed everything to the Trainer, which keeps care of growing our model and evaluates it from time to time\n","        trainer = Trainer(\n","            model=self.model,\n","            optimizer=optimizer,\n","            data_silo=data_silo,\n","            epochs=n_epochs,\n","            n_gpu=n_gpu,\n","            lr_schedule=lr_schedule,\n","            evaluate_every=evaluate_every,\n","            device=self.device,\n","        )\n","\n","        # 7. Let it grow! Watch the tracked metrics live on the public mlflow server: https://public-mlflow.deepset.ai\n","        trainer.train()\n","\n","        self.model.save(Path(save_dir), lm1_name=query_encoder_save_dir, lm2_name=passage_encoder_save_dir)\n","        self.query_tokenizer.save_pretrained(f\"{save_dir}/{query_encoder_save_dir}\")\n","        self.passage_tokenizer.save_pretrained(f\"{save_dir}/{passage_encoder_save_dir}\")\n","\n","    def save(self, save_dir: Union[Path, str], query_encoder_dir: str = \"query_encoder\",\n","             passage_encoder_dir: str = \"passage_encoder\"):\n","        \"\"\"\n","        Save DensePassageRetriever to the specified directory.\n","\n","        :param save_dir: Directory to save to.\n","        :param query_encoder_dir: Directory in save_dir that contains query encoder model.\n","        :param passage_encoder_dir: Directory in save_dir that contains passage encoder model.\n","        :return: None\n","        \"\"\"\n","        save_dir = Path(save_dir)\n","        self.model.save(save_dir, lm1_name=query_encoder_dir, lm2_name=passage_encoder_dir)\n","        save_dir = str(save_dir)\n","        self.query_tokenizer.save_pretrained(save_dir + f\"/{query_encoder_dir}\")\n","        self.passage_tokenizer.save_pretrained(save_dir + f\"/{passage_encoder_dir}\")\n","\n","    @classmethod\n","    def load(cls,\n","             load_dir: Union[Path, str],\n","             document_store: BaseDocumentStore,\n","             max_seq_len_query: int = 64,\n","             max_seq_len_passage: int = 256,\n","             use_gpu: bool = True,\n","             batch_size: int = 16,\n","             embed_title: bool = True,\n","             use_fast_tokenizers: bool = True,\n","             similarity_function: str = \"dot_product\",\n","             query_encoder_dir: str = \"query_encoder\",\n","             passage_encoder_dir: str = \"passage_encoder\"\n","             ):\n","        \"\"\"\n","        Load DensePassageRetriever from the specified directory.\n","        \"\"\"\n","\n","        load_dir = Path(load_dir)\n","        dpr = cls(\n","            document_store=document_store,\n","            query_embedding_model=Path(load_dir) / query_encoder_dir,\n","            passage_embedding_model=Path(load_dir) / passage_encoder_dir,\n","            max_seq_len_query=max_seq_len_query,\n","            max_seq_len_passage=max_seq_len_passage,\n","            use_gpu=use_gpu,\n","            batch_size=batch_size,\n","            embed_title=embed_title,\n","            use_fast_tokenizers=use_fast_tokenizers,\n","            similarity_function=similarity_function\n","        )\n","        logger.info(f\"DPR model loaded from {load_dir}\")\n","\n","        return dpr\n","\n","\n","class EmbeddingRetriever(BaseRetriever):\n","    def __init__(\n","        self,\n","        document_store: BaseDocumentStore,\n","        embedding_model: str,\n","        model_version: Optional[str] = None,\n","        use_gpu: bool = True,\n","        model_format: str = \"farm\",\n","        pooling_strategy: str = \"reduce_mean\",\n","        emb_extraction_layer: int = -1,\n","        top_k: int = 10,\n","    ):\n","        \"\"\"\n","        :param document_store: An instance of DocumentStore from which to retrieve documents.\n","        :param embedding_model: Local path or name of model in Hugging Face's model hub such as ``'deepset/sentence_bert'``\n","        :param model_version: The version of model to use from the HuggingFace model hub. Can be tag name, branch name, or commit hash.\n","        :param use_gpu: Whether to use gpu or not\n","        :param model_format: Name of framework that was used for saving the model. Options:\n","\n","                             - ``'farm'``\n","                             - ``'transformers'``\n","                             - ``'sentence_transformers'``\n","        :param pooling_strategy: Strategy for combining the embeddings from the model (for farm / transformers models only).\n","                                 Options:\n","\n","                                 - ``'cls_token'`` (sentence vector)\n","                                 - ``'reduce_mean'`` (sentence vector)\n","                                 - ``'reduce_max'`` (sentence vector)\n","                                 - ``'per_token'`` (individual token vectors)\n","        :param emb_extraction_layer: Number of layer from which the embeddings shall be extracted (for farm / transformers models only).\n","                                     Default: -1 (very last layer).\n","        :param top_k: How many documents to return per query.\n","        \"\"\"\n","\n","        # save init parameters to enable export of component config as YAML\n","        self.set_config(\n","            document_store=document_store, embedding_model=embedding_model, model_version=model_version,\n","            use_gpu=use_gpu, model_format=model_format, pooling_strategy=pooling_strategy,\n","            emb_extraction_layer=emb_extraction_layer, top_k=top_k,\n","        )\n","\n","        self.document_store = document_store\n","        self.model_format = model_format\n","        self.pooling_strategy = pooling_strategy\n","        self.emb_extraction_layer = emb_extraction_layer\n","        self.top_k = top_k\n","\n","        logger.info(f\"Init retriever using embeddings of model {embedding_model}\")\n","        if model_format == \"farm\" or model_format == \"transformers\":\n","            self.embedding_model = Inferencer.load(\n","                embedding_model, revision=model_version, task_type=\"embeddings\", extraction_strategy=self.pooling_strategy,\n","                extraction_layer=self.emb_extraction_layer, gpu=use_gpu, batch_size=4, max_seq_len=512, num_processes=0\n","            )\n","            # Check that document_store has the right similarity function\n","            similarity = document_store.similarity\n","            # If we are using a sentence transformer model\n","            if \"sentence\" in embedding_model.lower() and similarity != \"cosine\":\n","                logger.warning(f\"You seem to be using a Sentence Transformer with the {similarity} function. \"\n","                               f\"We recommend using cosine instead. \"\n","                               f\"This can be set when initializing the DocumentStore\")\n","            elif \"dpr\" in embedding_model.lower() and similarity != \"dot_product\":\n","                logger.warning(f\"You seem to be using a DPR model with the {similarity} function. \"\n","                               f\"We recommend using dot_product instead. \"\n","                               f\"This can be set when initializing the DocumentStore\")\n","\n","\n","        elif model_format == \"sentence_transformers\":\n","            try:\n","                from sentence_transformers import SentenceTransformer\n","            except ImportError:\n","                raise ImportError(\"Can't find package `sentence-transformers` \\n\"\n","                                  \"You can install it via `pip install sentence-transformers` \\n\"\n","                                  \"For details see https://github.com/UKPLab/sentence-transformers \")\n","            # pretrained embedding models coming from: https://github.com/UKPLab/sentence-transformers#pretrained-models\n","            # e.g. 'roberta-base-nli-stsb-mean-tokens'\n","            if use_gpu:\n","                device = \"cuda\"\n","            else:\n","                device = \"cpu\"\n","            self.embedding_model = SentenceTransformer(embedding_model, device=device)\n","            if document_store.similarity != \"cosine\":\n","                logger.warning(\n","                    f\"You are using a Sentence Transformer with the {document_store.similarity} function. \"\n","                    f\"We recommend using cosine instead. \"\n","                    f\"This can be set when initializing the DocumentStore\")\n","        else:\n","            raise NotImplementedError\n","\n","    def retrieve(self, query: str, filters: dict = None, top_k: Optional[int] = None, index: str = None) -> List[Document]:\n","        \"\"\"\n","        Scan through documents in DocumentStore and return a small number documents\n","        that are most relevant to the query.\n","\n","        :param query: The query\n","        :param filters: A dictionary where the keys specify a metadata field and the value is a list of accepted values for that field\n","        :param top_k: How many documents to return per query.\n","        :param index: The name of the index in the DocumentStore from which to retrieve documents\n","        \"\"\"\n","        if top_k is None:\n","            top_k = self.top_k\n","        if index is None:\n","            index = self.document_store.index\n","        query_emb = self.embed(texts=[query])\n","        documents = self.document_store.query_by_embedding(query_emb=query_emb[0], filters=filters,\n","                                                           top_k=top_k, index=index)\n","        return documents\n","\n","    def embed(self, texts: Union[List[List[str]], List[str], str]) -> List[np.ndarray]:\n","        \"\"\"\n","        Create embeddings for each text in a list of texts using the retrievers model (`self.embedding_model`)\n","\n","        :param texts: Texts to embed\n","        :return: List of embeddings (one per input text). Each embedding is a list of floats.\n","        \"\"\"\n","\n","        # for backward compatibility: cast pure str input\n","        if isinstance(texts, str):\n","            texts = [texts]\n","        assert isinstance(texts, list), \"Expecting a list of texts, i.e. create_embeddings(texts=['text1',...])\"\n","\n","        if self.model_format == \"farm\" or self.model_format == \"transformers\":\n","            # TODO: FARM's `sample_to_features_text` need to fix following warning -\n","            # tokenization_utils.py:460: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n","            emb = self.embedding_model.inference_from_dicts(dicts=[{\"text\": t} for t in texts])\n","            emb = [(r[\"vec\"]) for r in emb]\n","        elif self.model_format == \"sentence_transformers\":\n","            # texts can be a list of strings or a list of [title, text]\n","            # get back list of numpy embedding vectors\n","            emb = self.embedding_model.encode(texts, batch_size=200, show_progress_bar=False)\n","            emb = [r for r in emb]\n","        return emb\n","\n","    def embed_queries(self, texts: List[str]) -> List[np.ndarray]:\n","        \"\"\"\n","        Create embeddings for a list of queries. For this Retriever type: The same as calling .embed()\n","\n","        :param texts: Queries to embed\n","        :return: Embeddings, one per input queries\n","        \"\"\"\n","        return self.embed(texts)\n","\n","    def embed_passages(self, docs: List[Document]) -> Union[List[str], List[List[str]]]:\n","        \"\"\"\n","        Create embeddings for a list of passages. For this Retriever type: The same as calling .embed()\n","\n","        :param docs: List of documents to embed\n","        :return: Embeddings, one per input passage\n","        \"\"\"\n","        if self.model_format == \"sentence_transformers\":\n","            passages = [[d.meta[\"name\"] if d.meta and \"name\" in d.meta else \"\", d.text] for d in docs]  # type: ignore\n","        else:\n","            passages = [d.text for d in docs] # type: ignore\n","        return self.embed(passages)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YOqdpBj3O7bc"},"source":["## Train"]},{"cell_type":"code","metadata":{"id":"82nGzGCJLnya"},"source":["retriever = DensePassageRetriever(\n","    document_store=document_store,\n","    query_embedding_model=query_model,\n","    passage_embedding_model=passage_model,\n","    max_seq_len_query=32,\n","    max_seq_len_passage=256\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q2Ez1fQrLwv7"},"source":["retriever.train(\n","    data_dir=doc_dir,\n","    train_filename=train_filename,\n","    dev_filename=dev_filename,\n","    test_filename=dev_filename,\n","    n_epochs=1,\n","    batch_size=16,\n","    learning_rate=1e-06,\n","    grad_acc_steps=8,\n","    save_dir=save_dir,\n","    evaluate_every=1000,\n","    num_positives=1,\n","    num_hard_negatives=1\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1p8Q1EOfLzfv"},"source":["reloaded_retriever = DensePassageRetriever.load(load_dir=save_dir, document_store=document_store)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QJZbEiD6MOvP"},"source":["document_store.update_embeddings(reloaded_retriever, update_existing_embeddings=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tjlHhrcFCNYM"},"source":["from haystack.pipeline import DocumentSearchPipeline\n","from haystack.utils import print_documents\n","\n","p_retrieval = DocumentSearchPipeline(reloaded_retriever)\n","res = p_retrieval.run(\n","    query=\"\")\n","\n","print(res['documents'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DOYkM8fFCy3Z"},"source":["dummy_train_dataset = load_from_disk('/content/data/dummy_dataset/train')\n","\n","for num in range(200):\n","    if dummy_train_dataset['question'][num] in test_dataset['question']:\n","        print(dummy_train_dataset['id'][num],dummy_train_dataset['answers'][num]['text'][0])\n","        answer[dummy_train_dataset['id'][num]] = dummy_train_dataset['answers'][num]['text'][0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"klFLxxxoC0kk"},"source":["dummy_validation_dataset = load_from_disk('/content/data/dummy_dataset/validation')\n","\n","for num in range(20):\n","    if dummy_validation_dataset['question'][num] in test_dataset['question']:\n","        print(dummy_validation_dataset['id'][num],dummy_validation_dataset['answers'][num]['text'][0])\n","        answer[dummy_validation_dataset['id'][num]] = dummy_validation_dataset['answers'][num]['text'][0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aF-biEq9C4wX"},"source":["with open('/content/predictions.json', 'w') as f:\n","    json.dump(answer, f)"],"execution_count":null,"outputs":[]}]}