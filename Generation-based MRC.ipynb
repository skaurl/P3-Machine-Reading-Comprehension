{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Generation-based MRC.ipynb","provenance":[{"file_id":"1no8oN065EpQxnf5nM8aKNpqOrHBsAY_K","timestamp":1620720426545}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"fUuS4JiKRCM9"},"source":["# Generation-based MRC 문제를 풀어보기"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oaRbYnSouo_n","executionInfo":{"status":"ok","timestamp":1620980703138,"user_tz":-540,"elapsed":767,"user":{"displayName":"­김남혁 | 서울 수학과","photoUrl":"","userId":"02425184354385406709"}},"outputId":"2d6e70e5-bba5-4e76-af38-59410f2c085d"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fri May 14 08:25:02 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   51C    P0    56W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0sSJTt9ddUPi"},"source":["# Requirements\n","!pip install datasets\n","!pip install transformers\n","!pip install sentencepiece\n","!pip install nltk"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NP3y7Nea4r6q"},"source":["import nltk\n","nltk.download('punkt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zG2WUfDVhzuh"},"source":["!pip install git+https://github.com/SKT-AI/KoBART#egg=kobart"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9JLa6JZ6R6MF"},"source":["## 데이터 및 평가 지표 불러오기"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KMfeTcYNgS40","executionInfo":{"status":"ok","timestamp":1620980718452,"user_tz":-540,"elapsed":15994,"user":{"displayName":"­김남혁 | 서울 수학과","photoUrl":"","userId":"02425184354385406709"}},"outputId":"98bcffe8-a017-4cc0-cad7-9a5492eefefa"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QY6pVMWff-H9"},"source":["from datasets import load_from_disk\n","\n","train_dataset_origin = load_from_disk('/content/drive/MyDrive/Colab Notebooks/mrc/train')\n","\n","eval_dataset_origin = load_from_disk('/content/drive/MyDrive/Colab Notebooks/mrc/val')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fHMnn6-YSFs0"},"source":["from datasets import load_metric\n","\n","metric = load_metric('squad')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FzeR6uM1RibW"},"source":["## Pre-trained 모델 및 토크나이저 불러오기"]},{"cell_type":"code","metadata":{"id":"ZJoMcsnRRh4Y"},"source":["from transformers import BartForConditionalGeneration\n","from kobart import get_pytorch_kobart_model, get_kobart_tokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IreSlFmlIrIm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620980728752,"user_tz":-540,"elapsed":26253,"user":{"displayName":"­김남혁 | 서울 수학과","photoUrl":"","userId":"02425184354385406709"}},"outputId":"ea546677-0c5a-4fe1-8e80-558b1281c50b"},"source":["tokenizer = get_kobart_tokenizer()\n","\n","model = BartForConditionalGeneration.from_pretrained(get_pytorch_kobart_model())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["using cached model\n","using cached model\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AI3vIuHTSB10"},"source":["## 설정하기"]},{"cell_type":"code","metadata":{"id":"tjXjXtvnSAef"},"source":["max_source_length = 1024\n","max_target_length = 32\n","padding = False\n","preprocessing_num_workers = 8\n","batch_size = 8\n","num_train_epochs = 8"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3mLwSy1aShBT"},"source":["## 전처리하기"]},{"cell_type":"code","metadata":{"id":"YDD50QqYDdKr"},"source":["def preprocess_function(examples):\n","    inputs = [f'질문: {q}  문서: {c} </s>' for q, c in zip(examples['question'], examples['context'])]\n","    targets = [f'{a[\"text\"][0]} </s>' for a in examples['answers']]\n","    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n","\n","    # Setup the tokenizer for targets\n","    with tokenizer.as_target_tokenizer():\n","        labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    model_inputs[\"example_id\"] = []\n","    for i in range(len(model_inputs[\"labels\"])):\n","        model_inputs[\"example_id\"].append(examples[\"id\"][i])\n","    return model_inputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-b70jh26IrJS"},"source":["# column_names = datasets['train'].column_names\n","column_names = train_dataset_origin.column_names"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DDtsaJeVIrJT"},"source":["train_dataset = train_dataset_origin.map(\n","            preprocess_function,\n","            batched=True,\n","            num_proc=preprocessing_num_workers,\n","            remove_columns=column_names,\n","            load_from_cache_file=False\n","            )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XAUIoow83hQs"},"source":["eval_dataset = eval_dataset_origin.map(\n","            preprocess_function,\n","            batched=True,\n","            num_proc=preprocessing_num_workers,\n","            remove_columns=column_names,\n","            load_from_cache_file=False,\n","            )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"545PP3o8IrJV"},"source":["## Fine-tuning하기"]},{"cell_type":"code","metadata":{"id":"BtdkbBXRSnE_"},"source":["from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2xyccays3kjA"},"source":["label_pad_token_id = tokenizer.pad_token_id\n","\n","data_collator = DataCollatorForSeq2Seq(\n","            tokenizer,\n","            label_pad_token_id=label_pad_token_id,\n","            pad_to_multiple_of=None,\n","            )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fWEn9-bX3mAM"},"source":["import numpy as np\n","\n","def postprocess_text(preds, labels):\n","    preds = [pred.strip() for pred in preds]\n","    labels = [label.strip() for label in labels]\n","    \n","    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n","    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n","\n","    return preds, labels\n","\n","def compute_metrics(eval_preds):\n","    preds, labels = eval_preds\n","    if isinstance(preds, tuple):\n","        preds = preds[0]\n","    labels = np.where(labels == -100, 0, labels)\n","    \n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","    # decoded_labels is for rouge metric, not used for f1/em metric\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Some simple post-processing\n","    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n","\n","    formatted_predictions = [{\"id\": ex['id'], \"prediction_text\": decoded_preds[i]} for i, ex in enumerate(eval_dataset_origin)]\n","    references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in eval_dataset_origin]\n","\n","    result = metric.compute(predictions=formatted_predictions, references=references)\n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bliy8zgjIrJY"},"source":["args = Seq2SeqTrainingArguments(\n","    output_dir='/content/drive/MyDrive/Colab Notebooks/outputs',\n","    do_train=True,\n","    do_eval=True,\n","    predict_with_generate=True,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=num_train_epochs,\n","    evaluation_strategy='steps',\n","    eval_steps=500,\n","    save_steps=500,\n","    logging_steps=500,\n","    learning_rate=5e-5,\n","    save_total_limit=3,\n","    load_best_model_at_end = True,\n","    metric_for_best_model = 'exact_match',\n","    gradient_accumulation_steps = 16\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"imY1oC3SIrJf"},"source":["trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=args,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2J1AF3NFgFLl"},"source":["train_result = trainer.train()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Mz0ObFIaKTx"},"source":["# return tuple(save_directory)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kfAo4wLBDdKv"},"source":["train_result"],"execution_count":null,"outputs":[]}]}