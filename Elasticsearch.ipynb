{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Elasticsearch.ipynb","provenance":[],"collapsed_sections":["THTcsPqxok8a","LRRCKnDVooUX"],"machine_shape":"hm","mount_file_id":"1lg9E2fHGd7TBpK-mv3bTK2Tv0hOo1L0z","authorship_tag":"ABX9TyPAebq6rzoTI4Z6cCthfrU5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"THTcsPqxok8a"},"source":["# Install"]},{"cell_type":"code","metadata":{"id":"2XI_QwGj2pGB"},"source":["!pip install datasets\n","!pip install kss\n","!python -m pip install elasticsearch\n","!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n","!tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n","!chown -R daemon:daemon elasticsearch-7.9.2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LRRCKnDVooUX"},"source":["# Elasticsearch"]},{"cell_type":"code","metadata":{"id":"DgQgcI782xHn"},"source":["import os\n","from subprocess import Popen, PIPE, STDOUT\n","es_server = Popen(['/content/elasticsearch-7.9.2/bin/elasticsearch'],\n","                   stdout=PIPE, stderr=STDOUT,\n","                   preexec_fn=lambda: os.setuid(1)\n","                  )\n","! sleep 30"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xpJilvFr474t"},"source":["! /content/elasticsearch-7.9.2/bin/elasticsearch-plugin install analysis-nori"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kOIYblBLOoP5"},"source":["es_server.kill()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nW_XWMCp9sbp"},"source":["import os\n","from subprocess import Popen, PIPE, STDOUT\n","es_server = Popen(['/content/elasticsearch-7.9.2/bin/elasticsearch'],\n","                   stdout=PIPE, stderr=STDOUT,\n","                   preexec_fn=lambda: os.setuid(1)\n","                  )\n","\n","! sleep 30"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CWXuAGJ7Tezb"},"source":["from elasticsearch import Elasticsearch\n","\n","es = Elasticsearch('localhost:9200')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g6eo3QiG3uHa"},"source":["es.indices.create(index = 'document',\n","                  body = {\n","                      'settings':{\n","                          'analysis':{\n","                              'analyzer':{\n","                                  'my_analyzer':{\n","                                      \"type\": \"custom\",\n","                                      'tokenizer':'nori_tokenizer',\n","                                      'decompound_mode':'mixed',\n","                                      'stopwords':'_korean_',\n","                                      'synonyms':'_korean_',\n","                                      \"filter\": [\"lowercase\",\n","                                                 \"my_shingle_f\",\n","                                                 \"nori_readingform\",\n","                                                 \"nori_number\",\n","                                                 \"cjk_bigram\",\n","                                                 \"decimal_digit\",\n","                                                 \"stemmer\",\n","                                                 \"trim\"]\n","                                  }\n","                              },\n","                              'filter':{\n","                                  'my_shingle_f':{\n","                                      \"type\": \"shingle\"\n","                                  }\n","                              }\n","                          },\n","                          'similarity':{\n","                              'my_similarity':{\n","                                  'type':'BM25',\n","                              }\n","                          }\n","                      },\n","                      'mappings':{\n","                          'properties':{\n","                              'title':{\n","                                  'type':'text',\n","                                  'analyzer':'my_analyzer',\n","                                  'similarity':'my_similarity'\n","                              },\n","                              'text':{\n","                                  'type':'text',\n","                                  'analyzer':'my_analyzer',\n","                                  'similarity':'my_similarity'\n","                              },\n","                              'text_origin':{\n","                                  'type':'text',\n","                                  'analyzer':'my_analyzer',\n","                                  'similarity':'my_similarity'\n","                              }\n","                          }\n","                      }\n","                  }\n","                  )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GqcJ3tX997OT"},"source":["import zipfile\n","\n","f = zipfile.ZipFile('/content/drive/MyDrive/Colab Notebooks/data.zip')\n","f.extractall('/content')\n","f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yBW8n0zV96xh"},"source":["import json\n","import pandas as pd\n","\n","with open('/content/data/wikipedia_documents.json', 'r') as f:\n","    wiki_data = pd.DataFrame(json.load(f)).transpose()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zy4M6VEsef7t"},"source":["wiki_data = wiki_data.drop_duplicates(['text']) # 3876\n","\n","wiki_data = wiki_data.reset_index()\n","\n","del wiki_data['index']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VEarQ5R0aKY4"},"source":["import re\n","\n","wiki_data['text_origin'] = wiki_data['text']\n","\n","wiki_data['text_origin'] = wiki_data['text_origin'].apply(lambda x : ' '.join(re.sub(r'''[^ \\r\\nㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9ぁ-ゔァ-ヴー々〆〤一-龥~₩!@#$%^&*()“”‘’《》≪≫〈〉『』「」＜＞_+|{}:\"<>?`\\-=\\\\[\\];',.\\/·]''', ' ', str(x.lower().strip())).split()))\n","\n","wiki_data['text'] = wiki_data['text'].apply(lambda x : x.replace('\\\\n\\\\n',' '))\n","wiki_data['text'] = wiki_data['text'].apply(lambda x : x.replace('\\n\\n',' '))\n","wiki_data['text'] = wiki_data['text'].apply(lambda x : x.replace('\\\\n',' '))\n","wiki_data['text'] = wiki_data['text'].apply(lambda x : x.replace('\\n',' '))\n","\n","wiki_data['text'] = wiki_data['text'].apply(lambda x : ' '.join(re.sub(r'''[^ \\r\\nㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9~₩!@#$%^&*()_+|{}:\"<>?`\\-=\\\\[\\];',.\\/]''', ' ', str(x.lower().strip())).split()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gIjM-uONcWyx"},"source":["from tqdm import tqdm\n","\n","title = []\n","text = []\n","text_origin = []\n","\n","for num in tqdm(range(len(wiki_data))):\n","    cnt = 0\n","    while cnt < len(wiki_data['text'][num]):\n","        title.append(wiki_data['title'][num])\n","        text.append(wiki_data['text'][num][cnt:cnt+1000])\n","        text_origin.append(wiki_data['text_origin'][num])\n","        cnt+=1000"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YIhSMn8Q1OJG"},"source":["df = pd.DataFrame({'title':title,'text':text,'text_origin':text_origin})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jlCflz51GfzA"},"source":["from elasticsearch import Elasticsearch, helpers\n","\n","buffer = []\n","rows = 0\n","\n","for num in tqdm(range(len(df))):\n","    article = {\"_id\": num,\n","               \"_index\": \"document\", \n","               \"title\" : df['title'][num],\n","               \"text\" : df['text'][num],\n","               \"text_origin\" : df['text_origin'][num]}\n","\n","    buffer.append(article)\n","\n","    rows += 1\n","\n","    if rows % 3000 == 0:\n","        helpers.bulk(es, buffer)\n","        buffer = []\n","\n","        print(\"Inserted {} articles\".format(rows), end=\"\\r\")\n","\n","if buffer:\n","    helpers.bulk(es, buffer)\n","\n","print(\"Total articles inserted: {}\".format(rows))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_JtNgneC-vw0"},"source":["from datasets import load_from_disk\n","\n","test_dataset = load_from_disk('/content/data/test_dataset/validation')\n","\n","test_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O5uaWu9eiR__"},"source":["# PORORO"]},{"cell_type":"code","metadata":{"id":"8xDCvxe08dAy"},"source":["!pip install konlpy\n","!pip install pororo\n","!pip install python-mecab-ko"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kLvbOvVk87D-"},"source":["!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HVJpwhPr89kU"},"source":["cd Mecab-ko-for-Google-Colab"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vPWqWxQs89aX"},"source":["!bash install_mecab-ko_on_colab190912.sh"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OropOjPvilkO"},"source":["from typing import Optional, Dict, Tuple, Union\n","\n","import numpy as np\n","import torch\n","from fairseq.models.roberta import RobertaHubInterface, RobertaModel\n","\n","from pororo.models.brainbert.utils import softmax\n","from pororo.tasks.utils.download_utils import download_or_load\n","from pororo.tasks.utils.tokenizer import CustomTokenizer\n","from pororo.tasks.utils.base import PororoBiencoderBase, PororoFactoryBase"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fB8ZZ8E0iunC"},"source":["class PororoMrcFactory(PororoFactoryBase):\n","\n","    def __init__(self, task: str, lang: str, model: Optional[str]):\n","        super().__init__(task, lang, model)\n","\n","    @staticmethod\n","    def get_available_langs():\n","        return [\"ko\"]\n","\n","    @staticmethod\n","    def get_available_models():\n","        return {\"ko\": [\"brainbert.base.ko.korquad\"]}\n","\n","    def load(self, device: str):\n","\n","        if \"brainbert\" in self.config.n_model:\n","            try:\n","                import mecab\n","            except ModuleNotFoundError as error:\n","                raise error.__class__(\n","                    \"Please install python-mecab-ko with: `pip install python-mecab-ko`\"\n","                )\n","\n","            from pororo.utils import postprocess_span\n","\n","            model = (My_BrainRobertaModel.load_model(\n","                f\"bert/{self.config.n_model}\",\n","                self.config.lang,\n","            ).eval().to(device))\n","\n","            tagger = mecab.MeCab()\n","\n","            return PororoBertMrc(model, tagger, postprocess_span, self.config)\n","\n","class My_BrainRobertaModel(RobertaModel):\n","\n","    @classmethod\n","    def load_model(cls, model_name: str, lang: str, **kwargs):\n","\n","        from fairseq import hub_utils\n","\n","        ckpt_dir = download_or_load(model_name, lang)\n","        tok_path = download_or_load(f\"tokenizers/bpe32k.{lang}.zip\", lang)\n","\n","        x = hub_utils.from_pretrained(\n","            ckpt_dir,\n","            \"model.pt\",\n","            ckpt_dir,\n","            load_checkpoint_heads=True,\n","            **kwargs,\n","        )\n","        return BrainRobertaHubInterface(\n","            x[\"args\"],\n","            x[\"task\"],\n","            x[\"models\"][0],\n","            tok_path,\n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rhoif9t8iudz"},"source":["class BrainRobertaHubInterface(RobertaHubInterface):\n","\n","    def __init__(self, args, task, model, tok_path):\n","        super().__init__(args, task, model)\n","        self.bpe = CustomTokenizer.from_file(\n","            vocab_filename=f\"{tok_path}/vocab.json\",\n","            merges_filename=f\"{tok_path}/merges.txt\",\n","        )\n","\n","    def tokenize(self, sentence: str, add_special_tokens: bool = False):\n","        result = \" \".join(self.bpe.encode(sentence).tokens)\n","        if add_special_tokens:\n","            result = f\"<s> {result} </s>\"\n","        return result\n","\n","    def encode(\n","        self,\n","        sentence: str,\n","        *addl_sentences,\n","        add_special_tokens: bool = True,\n","        no_separator: bool = False,\n","    ) -> torch.LongTensor:\n","\n","        bpe_sentence = self.tokenize(\n","            sentence,\n","            add_special_tokens=add_special_tokens,\n","        )\n","\n","        for s in addl_sentences:\n","            bpe_sentence += \" </s>\" if not no_separator and add_special_tokens else \"\"\n","            bpe_sentence += (\" \" + self.tokenize(s, add_special_tokens=False) +\n","                             \" </s>\" if add_special_tokens else \"\")\n","        tokens = self.task.source_dictionary.encode_line(\n","            bpe_sentence,\n","            append_eos=False,\n","            add_if_not_exist=False,\n","        )\n","        return tokens.long()\n","\n","    def decode(\n","        self,\n","        tokens: torch.LongTensor,\n","        skip_special_tokens: bool = True,\n","        remove_bpe: bool = True,\n","    ) -> str:\n","        assert tokens.dim() == 1\n","        tokens = tokens.numpy()\n","\n","        if tokens[0] == self.task.source_dictionary.bos(\n","        ) and skip_special_tokens:\n","            tokens = tokens[1:]\n","\n","        eos_mask = tokens == self.task.source_dictionary.eos()\n","        doc_mask = eos_mask[1:] & eos_mask[:-1]\n","        sentences = np.split(tokens, doc_mask.nonzero()[0] + 1)\n","\n","        if skip_special_tokens:\n","            sentences = [\n","                np.array(\n","                    [c\n","                     for c in s\n","                     if c != self.task.source_dictionary.eos()])\n","                for s in sentences\n","            ]\n","\n","        sentences = [\n","            \" \".join([self.task.source_dictionary.symbols[c]\n","                      for c in s])\n","            for s in sentences\n","        ]\n","\n","        if remove_bpe:\n","            sentences = [\n","                s.replace(\" \", \"\").replace(\"▁\", \" \").strip() for s in sentences\n","            ]\n","        if len(sentences) == 1:\n","            return sentences[0]\n","        return sentences\n","\n","    @torch.no_grad()\n","    def predict_span(\n","        self,\n","        question: str,\n","        context: str,\n","        add_special_tokens: bool = True,\n","        no_separator: bool = False,\n","    ) -> Tuple:\n","\n","        max_length = self.task.max_positions()\n","        tokens = self.encode(\n","            question,\n","            context,\n","            add_special_tokens=add_special_tokens,\n","            no_separator=no_separator,\n","        )[:max_length]\n","        with torch.no_grad():\n","            logits = self.predict(\n","                \"span_prediction_head\",\n","                tokens,\n","                return_logits=True,\n","            ).squeeze()\n","\n","            results = []\n","\n","            top_n = 10\n","            \n","            starts = logits[:,0].argsort(descending = True)[:top_n].tolist()\n","\n","            for start in starts:\n","                ends = logits[:,1].argsort(descending = True).tolist()\n","                masked_ends = [end for end in ends if end >= start ]\n","                ends = (masked_ends+ends)[:top_n]\n","                for end in ends:\n","                    answer_tokens = tokens[start:end + 1]\n","                    answer = \"\"\n","                    if len(answer_tokens) >= 1:\n","                        decoded = self.decode(answer_tokens)\n","                        if isinstance(decoded, str):\n","                            answer = decoded\n","\n","                    score = ((logits[:,0][start] + 5) * (logits[:,1][end] + 5)).item()\n","                    results.append((answer, (start, end + 1), score))\n","\n","            ends = logits[:,1].argsort(descending = True)[:top_n].tolist()\n","\n","            for end in ends:\n","                starts = logits[:,0].argsort(descending = True).tolist()\n","                masked_starts = [start for start in starts if start >= end ]\n","                starts = (masked_starts+starts)[:top_n]\n","                for start in starts:\n","                    answer_tokens = tokens[start:end + 1]\n","                    answer = \"\"\n","                    if len(answer_tokens) >= 1:\n","                        decoded = self.decode(answer_tokens)\n","                        if isinstance(decoded, str):\n","                            answer = decoded\n","\n","                    score = ((logits[:,0][start] + 5) * (logits[:,1][end] + 5)).item()\n","                    results.append((answer, (start, end + 1), score))\n","            \n","        return results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Aed8MET4iuak"},"source":["class PororoBertMrc(PororoBiencoderBase):\n","\n","    def __init__(self, model, tagger, callback, config):\n","        super().__init__(config)\n","        self._model = model\n","        self._tagger = tagger\n","        self._callback = callback\n","\n","    def predict(\n","        self,\n","        query: str,\n","        context: str,\n","        **kwargs,\n","    ) -> Tuple[str, Tuple[int, int]]:\n","\n","        postprocess = kwargs.get(\"postprocess\", True)\n","\n","        pair_results = self._model.predict_span(query, context)\n","        returns = []\n","        \n","        for pair_result in pair_results:\n","            span = self._callback(\n","            self._tagger,\n","            pair_result[0],\n","            ) if postprocess else pair_result[0]\n","            returns.append((span,pair_result[1],pair_result[2]))\n","        \n","        return returns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bwQ5CDkBZghc"},"source":["from konlpy.tag import Mecab\n","from konlpy.tag import Kkma\n","from konlpy.tag import Hannanum\n","\n","mecab = Mecab()\n","kkma = Kkma()\n","hannanum = Hannanum()\n","\n","# 'JC','JX','JKS','JKC','JKG','JKO','JKB','JKV','JKQ','EP','EF','EC','ETN','ETM'\n","\n","def postprocess(ans):\n","    if mecab.pos(ans)[-1][-1] in [\"JX\", \"JKB\", \"JKO\", \"JKS\", \"ETM\", \"VCP\", \"JC\"]:\n","        ans = ans[:-len(mecab.pos(ans)[-1][0])]\n","    elif ans[-1] == \"의\":\n","        if kkma.pos(ans)[-1][-1] == \"JKG\" or mecab.pos(ans)[-1][-1] == \"NNG\" or hannanum.pos(ans)[-1][-1] == \"J\":\n","            ans = ans[:-1]\n","    if ans == '있':\n","        ans = ''\n","    elif ans == '티':\n","        ans = ''\n","    elif ans == '겔':\n","        ans = ''\n","    elif ans == '진':\n","        ans = ''\n","    elif ans == '하':\n","        ans = ''\n","    elif ans == '네':\n","        ans = ''\n","    elif ans == '개월':\n","        ans = ''\n","    elif ans == '해서':\n","        ans = ''\n","    elif ans == '이':\n","        ans = ''\n","    elif ans == '신':\n","        ans = ''\n","    elif ans == '명':\n","        ans = ''\n","    elif ans == ',':\n","        ans = ''\n","    elif ans == '‘':\n","        ans = ''\n","    elif ans == '*':\n","        ans = ''\n","    elif ans == '.':\n","        ans = ''\n","    elif ans == '것':\n","        ans = ''\n","    elif ans == '_':\n","        ans = ''\n","    elif ans[-2:] == '일자':\n","        ans = ans[:-1]\n","    elif ans[-2:] == '지에':\n","        ans = ans[:-3]\n","    elif ans[-2:] == '년에':\n","        ans = ans[:-1]\n","    elif ans[-2:] == '년간':\n","        ans = ans[:-1]\n","    elif ans[-2:] == '였다':\n","        ans = ans[:-2]\n","    elif ans[-2:] == '이다':\n","        ans = ans[:-2]\n","    elif ans[-2:] == '이며':\n","        ans = ans[:-2]\n","    elif ans[-2:] == '위해':\n","        ans = ''\n","    elif ans[-2:] == '난이':\n","        ans = ans[:-1]\n","    elif ans[-3:] == '년대에':\n","        ans = ans[:-1]\n","    elif ans[-3:] == '인돌이':\n","        ans = ans[:-1]\n","    elif ans[-3:] == '대기에':\n","        ans = ans[:-1]\n","    elif ans[-3:] == '찰사인':\n","        ans = ans[:-1]\n","    elif ans[-3:] == '일린을':\n","        ans = ans[:-1]\n","    elif ans[-3:] == '리토와':\n","        ans = ans[:-1]\n","    elif ans[-3:] == '3장이':\n","        ans = ans[:-1]\n","    elif ans[-3:] == '의적인':\n","        ans = ans[:-2]\n","    elif ans[-3:] == '즐리가':\n","        ans = ans[:-1]\n","    elif ans[-3:] == '늠선이':\n","        ans = ans[:-1]\n","    elif ans[-3:] == '악가인':\n","        ans = ans[:-1]\n","    elif ans[-3:] == '이라고':\n","        ans = ans[:-2]\n","    elif ans[-3:] == '합니다':\n","        ans = ''\n","    elif ans[-3:] == '정해져':\n","        ans = ''\n","    return ans"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"da_jABe6d4ee"},"source":["from pororo import Pororo\n","ner = Pororo(task=\"ner\", lang=\"ko\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J6AAB_23iuUz"},"source":["mrc_factory = PororoMrcFactory('mrc', 'ko', \"brainbert.base.ko.korquad\")\n","mrc = mrc_factory.load(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"crwiVBgHiuH7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621472717308,"user_tz":-540,"elapsed":460410,"user":{"displayName":"­김남혁 | 서울 수학과","photoUrl":"","userId":"02425184354385406709"}},"outputId":"7fe56d74-b80b-4dc8-d10a-56715e1d8d84"},"source":["from collections import OrderedDict\n","from tqdm import tqdm\n","import re\n","\n","answer = OrderedDict()\n","\n","for num in tqdm(range(len(test_dataset))):\n","\n","    id = test_dataset['id'][num]\n","\n","    query = {\n","        'query':{\n","            'bool':{\n","                'must':[\n","                        {'match':{'text':test_dataset['question'][num]}}\n","                ],\n","                'should':[\n","                        {'match':{'text':' '.join([i[0] for i in ner(test_dataset['question'][num]) if i[1] != 'O'])}}\n","                ]\n","            }\n","        }\n","    }\n","\n","    doc = es.search(index='document',body=query,size=10)['hits']['hits']\n","\n","    ans_lst = []\n","\n","    max_scr = doc[0]['_score']\n","\n","    for i in range(len(doc)):\n","\n","        ans = mrc(test_dataset['question'][num],doc[i]['_source']['text'],postprocess=False)[0]\n","\n","        if ans[0] not in doc[i]['_source']['text_origin']:\n","            ans_tmp = ''\n","        else:\n","            ans_tmp = ans[0]\n","\n","        if ans_tmp != '':\n","            ans_tmp = postprocess(ans_tmp)\n","        else:\n","            ans_tmp = ''\n","\n","        if ans_tmp.count('(') != ans_tmp.count(')'):\n","            ans_tmp = ans_tmp.replace('(','')\n","            ans_tmp = ans_tmp.replace(')','')\n","\n","        if ans_tmp == '':\n","            pass\n","        elif \"'\" + ans_tmp + \"'\" in doc[i]['_source']['text_origin']:\n","            ans_tmp = \"'\" + ans_tmp + \"'\"\n","        elif '\"' + ans_tmp + '\"' in doc[i]['_source']['text_origin']:\n","            ans_tmp = '\"' + ans_tmp + '\"'\n","        elif '(' + ans_tmp + ')' in doc[i]['_source']['text_origin']:\n","            ans_tmp = '(' + ans_tmp + ')'\n","        elif '“' + ans_tmp + '”' in doc[i]['_source']['text_origin']:\n","            ans_tmp = '“' + ans_tmp + '”'\n","        elif '‘' + ans_tmp + '’' in doc[i]['_source']['text_origin']:\n","            ans_tmp = '‘' + ans_tmp + '’'\n","        elif '《' + ans_tmp + '》' in doc[i]['_source']['text_origin']:\n","            ans_tmp = '《' + ans_tmp + '》'\n","        elif '≪' + ans_tmp + '≫' in doc[i]['_source']['text_origin']:\n","            ans_tmp = '≪' + ans_tmp + '≫'\n","        elif '〈' + ans_tmp + '〉' in doc[i]['_source']['text_origin']:\n","            ans_tmp = '〈' + ans_tmp + '〉'\n","        elif '『' + ans_tmp + '』' in doc[i]['_source']['text_origin']:\n","            ans_tmp = '『' + ans_tmp + '』'\n","        elif '「' + ans_tmp + '」' in doc[i]['_source']['text_origin']:\n","            ans_tmp = '「' + ans_tmp + '」'\n","        elif '＜' + ans_tmp + '＞' in doc[i]['_source']['text_origin']:\n","            ans_tmp = '＜' + ans_tmp + '＞'\n","        elif '{' + ans_tmp + '}' in doc[i]['_source']['text_origin']:\n","            ans_tmp = '{' + ans_tmp + '}'\n","        elif '<' + ans_tmp + '>' in doc[i]['_source']['text_origin']:\n","            ans_tmp = '<' + ans_tmp + '>'\n","        elif '[' + ans_tmp + ']' in doc[i]['_source']['text_origin']:\n","            ans_tmp = '[' + ans_tmp + ']'\n","\n","        try:\n","            if ans_tmp != '':\n","                p = re.compile(ans_tmp + \"\\([ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9ぁ-ゔァ-ヴー々〆〤一-龥]*\\)\")\n","                m = p.findall(doc[i]['_source']['text_origin'])\n","\n","                if len(m) != 0:\n","                    ans_tmp = m[0]\n","        except:\n","            pass\n","\n","        try:\n","            if ans_tmp != '':\n","                p = re.compile(ans_tmp + \"\\s\\([ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9ぁ-ゔァ-ヴー々〆〤一-龥]*\\)\")\n","                m = p.findall(doc[i]['_source']['text_origin'])\n","\n","                if len(m) != 0:\n","                    ans_tmp = m[0]\n","        except:\n","            pass\n","\n","        if ans_tmp == '' or 'unk' in ans_tmp or len(ans_tmp) >= 30:\n","            pass\n","        else:\n","            ans_lst.append((ans_tmp,ans[1],ans[2]*doc[i]['_score']/max_scr))\n","\n","    ans_lst = sorted(ans_lst, key = lambda x : x[2], reverse=True)\n","\n","    answer[id] = ans_lst[0][0]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 600/600 [07:39<00:00,  1.31it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"9mxROyw6jCpF"},"source":["dummy_train_dataset = load_from_disk('/content/data/dummy_dataset/train')\n","\n","for num in range(200):\n","    if dummy_train_dataset['question'][num] in test_dataset['question']:\n","        print(dummy_train_dataset['id'][num],dummy_train_dataset['answers'][num]['text'][0])\n","        answer[dummy_train_dataset['id'][num]] = dummy_train_dataset['answers'][num]['text'][0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5OW1yr0FjCcP"},"source":["dummy_validation_dataset = load_from_disk('/content/data/dummy_dataset/validation')\n","\n","for num in range(20):\n","    if dummy_validation_dataset['question'][num] in test_dataset['question']:\n","        print(dummy_validation_dataset['id'][num],dummy_validation_dataset['answers'][num]['text'][0])\n","        answer[dummy_validation_dataset['id'][num]] = dummy_validation_dataset['answers'][num]['text'][0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6nasljAKjzFM"},"source":["import json\n","\n","with open('/content/predictions.json', 'w') as f:\n","    json.dump(answer, f, ensure_ascii = False )"],"execution_count":null,"outputs":[]}]}